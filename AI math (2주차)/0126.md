# 경사하강법(순한맛)

- 미분이란? 

  - 미분은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구, **최적화**에서 가장많이 사용

    ```
    ##미분
    
    import sympy as sym
    from sympy.abc import x
    
    sym.diff(sym.poly(x**2+2*x+3),x)
    
    >> Poly(2*x + 2, x, domain='ZZ')
    ```

- 미분이 쓰이는 곳

  - 미분은 함수 f의 주어진 점(x,f(x))에서의 **접선의 기울기**를 구한다.
  - 현재 위치에서 미분값을 더해(경사 상승법) 극대값의 위치를 구한다.
  - 현재 위치에서 미분값을 빼(경사 하강법) 극소값의 위치를 구한다.



- 경사하강법

  - 함수의 기울기(경사)를 구하여 기울기가 낮은 쪽으로 계속 이동시켜 극값(최적값)에 이를 때까지 반복하는 것

    ```
    ## example
    
    def func(val):
      fun  = sym.poly(x**2+2*x+3)
      return fun.subs(x,val),fun
    
    
    def func_gradient(fun,val):
      _, function = fun(val)
      diff = sym.diff(function,x)
      return diff.subs(x,val), diff
    
    
    def gradient_descent(fun,init_point, lr = 1e-2, epsilon= 1e-5):
      cnt = 0
      val = init_point
      diff, _ = func_gradient(fun, init_point)
      while np.abs(diff)>epsilon:
        val = val - lr*diff
        diff, _ = func_gradient(fun,val)
        cnt+=1
    
      print("함수: {}, 연산횟수: {}, 최소점: ( {}, {})".format(fun(val)[1],cnt,val,fun(val)[0]))
    
    gradient_descent(fun=func, init_point=np.random.uniform(-2,2))
    
    >> 함수: Poly(x**2 + 2*x + 3, x, domain='ZZ'), 연산횟수: 653, 최소점: ( -0., 2.00000000002456)
    
    ```

  - 변수가 벡터일 경우?

    - 편미분을 사용한다.

      ```
      ## example
      
      import sympy as sym
      from sympy.abc import x,y
      
      sym.diff(sym.poly(x**2+2*x*y+3)+sym.cos(x + 2*y),x)
      
      >> 2*x + 2*y - sin(x + 2*y)
      ```

    - 각 변수 별로 편미분을 계산(그레디언트벡터)하여 경사하강/상승에 사용

      <a href="https://ibb.co/YNXhCML"><img src="https://i.ibb.co/FhXzrMD/2021-01-26-12-39-39.png" alt="2021-01-26-12-39-39" border="0"></a>

      



# 경사하강법(매운맛)

- 선형회귀분석 복습

  <a href="https://ibb.co/ykpvNFs"><img src="https://i.ibb.co/KsD45xy/2021-01-31-16-02-42.png" alt="2021-01-31-16-02-42" border="0"></a>

  이전 시간에 **무어펠로즈 역행렬**을 이용하여 선형회귀식을 찾았으나 선형모델에서만 가능하다는 한계가 있기 때문에 좀 더 일반적으로 쓰이는 **경사하강법**을 이용해서 선형회귀식을 찾아보려고 한다.

- 경사하강법으로 선형회귀 계수 구하기

  <a href="https://ibb.co/MPdqL4k"><img src="https://i.ibb.co/j3t7F2k/2021-01-26-12-49-49.png" alt="2021-01-26-12-49-49" border="0"></a>

  ```
  ##경사하강법 기반 선형회귀 알고리즘 example
  
  X = np.array([[1,1],[1,2],[2,2],[2,3]])
  Y = np.dot(X,np.array([1,2])) + 3
  
  beta_gd = [10.1,15.1,-6.5] #[1,2,3]이 정답
  X_=np.array([np.append(x,[1])for x in X])
  X_
  
  >>
  array([[1, 1, 1],
         [1, 2, 1],
         [2, 2, 1],
         [2, 3, 1]])
         
  for i in range(5000):
  	error = y-X_ @ beta_gd
  	grad = -np.transpose(X_) @ error
  	beta_gd = beta_gd - 0.01 * grad
  print(beta_gd)
  
  >>[1.00000356, 1.9999595, 2.999999652]
  ```



- 경사하강법의 한계
  - 경사하강법은 미분이 가능하고 볼록(convex)한 함수에 대해서 수렴이 보장되있다.
  
    - convex한 함수란?
  
      - 함수 f(x)가 있을 때, 특정 두 지점을 찍었을 때, x축을 기준으로 t라는 비율로 된 곳의 값을 함수 f에 넣은 것은 각각의 함수 값 f(x)와 f(y)에 대해서 같은 비율을 적용한 것 보다 적은 값을 가지는 것이 볼록 함수이다.
  
        <a href="https://imgbb.com/"><img src="https://i.ibb.co/1GcSnfN/2021-01-31-16-04-36.png" alt="2021-01-31-16-04-36" border="0"></a>
  
        
  
  - 특히 선현회귀의 목적식 ||y-Xb||가 회귀 계수 b에 대해 볼록함수이기 때문에 수렴이 보장된다.
  
  - 하지만 비선형회귀에선 목적식이 볼록하지 않을 수 있으므로 수렴이 보장되지 않는다.



- 확률적 경사하강법(SGD)

  - 확률적 경사하강법은 데이터 한개 또는 일부를 활용하여 업데이트한다.
  - non-convex한 목적식은 SGD를 이용해 최적화가 가능하다.

  

- 확률적 경사하강법의 원리

  <a href="https://ibb.co/jDf4qGM"><img src="https://i.ibb.co/ZxXTvK8/2021-01-26-19-00-45.png" alt="2021-01-26-19-00-45" border="0"></a>