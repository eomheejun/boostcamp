# Transformer

Transformer 모델에 대해서 3주차 0204.md파일에 개념적인 부분을 적어 놨기 때문에 이전에 배웠던 부분에 대해서 보충 설명 및 Multi-Head Attention 코드에 대해 학습한다.



- Attention과 Self-Attention의 차이

  - Seq2Seq에서의 Attention는 Decoder의 특정 time step의 output이 Encoder의 모든 time step의 output중 어떤 time step과 가장 연관(dot, general, cocat 등과 같은 방법으로)이 있는지 알아볼 수 있었다. 

  - Self-Attention은 위와 같이 Decoder의 특정 time step의 output이 Encoder의 모든 time step의 output간의 관계가 아닌 Encoder 혹은 Decoder 내에서 각 단어들이 자기 자신을 포함하여 모든 입력값들에 대해 어떤 연관관계를 가지는지 알아보는 방법이고 이 메커니즘이 Transformer의 핵심 이다.



- Self-Attention

  <a href="https://ibb.co/mvLQcs6"><img src="https://i.ibb.co/vs72j5Z/2021-02-18-21-19-32.png" alt="2021-02-18-21-19-32" border="0"></a>

  각 word의 embedding vector로 Q,K,V값을 구해 위와같은 Attention 식을 거쳐 각 word의 encoding vector를 구한다. 그리고 이 encoding vector로 더 많은 word의 속성을 만들기 위해 Self-Attention을 여러번 반복하는 것을 Multi-Head Attention이라고 한다.



- Multi-Head Attention

  ```
  from torch import nn
  from torch.nn import functional as F
  from tqdm import tqdm
  
  import torch
  import math
  
  pad_id = 0
  vocab_size = 100
  
  data = [
    [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],
    [60, 96, 51, 32, 90],
    [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],
    [75, 51],
    [66, 88, 98, 47],
    [21, 39, 10, 64, 21],
    [98],
    [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],
    [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],
    [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]
  ]
  
  ```

  ```
  def padding(data):
    max_len = len(max(data, key=len))
    print(f"Maximum sequence length: {max_len}")
  
    for i, seq in enumerate(tqdm(data)):
      if len(seq) < max_len:
        data[i] = seq + [pad_id] * (max_len - len(seq))
  
    return data, max_len
  ```

  padding 이라는 함수를 이용해 각 단어들의 차원을 맞춰주었다. 10개의 단어 중 가장 긴 길이를 가진 8번째 vector에 맞춰서 모자란 차원을 0으로 채워 나갔다.

  ```
  d_model = 512  # model의 hidden size
  num_heads = 8  # head의 개수
  
  embedding = nn.Embedding(vocab_size, d_model)
  
  # B: batch size, L: maximum sequence length
  batch = torch.LongTensor(data)  # (B, L)
  batch_emb = embedding(batch)  # (B, L, d_model)
  ```

  d_model은 가중치 행렬 W의 크기가 되고 입력이 주어졌을때 W가 곱해져 Q,K,V의 마지막차원의 크기를 512로 맞춰 주었다. num_heads가 바로 self Attention을 통해 각 단어당 8개의 Q,K,V vector를 구한다는 뜻이다.  embedding 함수를 통해 batch_emb가 각 배치사이즈 별로 단어들의 embedding vector가 되었다.

  ```
  w_q = nn.Linear(d_model, d_model)
  w_k = nn.Linear(d_model, d_model)
  w_v = nn.Linear(d_model, d_model)
  
  w_0 = nn.Linear(d_model, d_model)
  
  q = w_q(batch_emb)  # (B, L, d_model)
  k = w_k(batch_emb)  # (B, L, d_model)
  v = w_v(batch_emb)  # (B, L, d_model)
  
  print(q.shape)
  print(k.shape)
  print(v.shape)
  
  >>
  torch.Size([10, 20, 512])
  torch.Size([10, 20, 512])
  torch.Size([10, 20, 512])
  ```

  위에서 512X512짜리의 Q,K,V를 만드는 3개의 가중치 행렬 W를 선언하고 W_0를 이용해 각 encoding vector를 concat하여 W_0를 곱해 마지막에 다시 처음 차원을 맞춰주게 된다.

  <a href="https://ibb.co/nQ3xgXk"><img src="https://i.ibb.co/KVNT0ty/2021-02-18-21-40-42.png" alt="2021-02-18-21-40-42" border="0"></a>

  일반적으로 Multi head attention이라고 하면 Self-Attention반복을 통해 Q,K,V를 여러개 만드는 개념을 뜻하지만 메모리낭비로 인해 위의 코드와 같이 Q,K,V를 구한 뒤 num_heads만큼 나눠주어 같은 역할을 하게 끔 코드를 작성한다.

  ```
  batch_size = q.shape[0]
  d_k = d_model // num_heads
  
  q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
  k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
  v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
  
  print(q.shape)
  print(k.shape)
  print(v.shape)
  
  >>
  
  torch.Size([10, 20, 8, 64])
  torch.Size([10, 20, 8, 64])
  torch.Size([10, 20, 8, 64])
  ```

  따라서 위와 같이 d_model // num_heads를 Q,K vector의 차원으로 지정해주어 512짜리의 길이를 64의 길이만큼 8개로 사용하여 Multi-Head Attention을 구현한다.

  ```
  q = q.transpose(1, 2)  # (B, num_heads, L, d_k)
  k = k.transpose(1, 2)  # (B, num_heads, L, d_k)
  v = v.transpose(1, 2)  # (B, num_heads, L, d_k)
  
  print(q.shape)
  print(k.shape)
  print(v.shape)
  
  >>
  
  torch.Size([10, 8, 20, 64])
  torch.Size([10, 8, 20, 64])
  torch.Size([10, 8, 20, 64])
  ```

  위와 같이 transpose를 통해 차원의 순서를 batch, num_heads, word_Length, Q,K vector의 크기로 맞춰준다.

  ```
  attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)
  attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)
  ```

  가장 최초의 Self-Attention 수식을 코드로 구현하여 softmax함수를 적용하고 

  ```
  attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)
  
  print(attn_values.shape)
  
  >>
  torch.Size([10, 8, 20, 64])
  ```

  다시 V vector를 곱해주어 최종적인 encoding vector를 구했다.

  ```
  attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)
  attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)
  
  print(attn_values.shape)
  
  outputs = w_0(attn_values)
  ```

  마지막으로 그 8개의 encoding vector를 concat하여 최종적인 encoding vector를 구했다.

  ```
  class MultiheadAttention(nn.Module):
    def __init__(self):
      super(MultiheadAttention, self).__init__()
  
      # Q, K, V learnable matrices
      self.w_q = nn.Linear(d_model, d_model)
      self.w_k = nn.Linear(d_model, d_model)
      self.w_v = nn.Linear(d_model, d_model)
  
      # Linear transformation for concatenated outputs
      self.w_0 = nn.Linear(d_model, d_model)
  
    def forward(self, q, k, v):
      batch_size = q.shape[0]
  
      q = self.w_q(q)  # (B, L, d_model)
      k = self.w_k(k)  # (B, L, d_model)
      v = self.w_v(v)  # (B, L, d_model)
  
      q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
      k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
      v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
  
      q = q.transpose(1, 2)  # (B, num_heads, L, d_k)
      k = k.transpose(1, 2)  # (B, num_heads, L, d_k)
      v = v.transpose(1, 2)  # (B, num_heads, L, d_k)
  
      attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)
      attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)
  
      return self.w_0(attn_values)
  
    def self_attention(self, q, k, v):
      attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)
      attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)
  
      attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)
  
      return attn_values
  ```

  ```
  multihead_attn = MultiheadAttention()
  
  outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)
  ```

  위의 작업을 class로 나타내어 최종적인 Multi-Head Attention 함수를 만들어 냈다.

  



- Multi-Head Attention with mask

  - Decoder에서 현재 word까지만 예측을 할 수 있도록 미래의 값을 mask처리를 해준다. mask가 있느냐 없느냐를 코드로 구현해 Encoder Decoder에서 모두 Multi-Head Attention을 사용할 수 있게 할 수 있다.

    

  ```
  pad_id = 0
  vocab_size = 100
  
  data = [
    [62, 13, 47, 39, 78, 33, 56, 13],
    [60, 96, 51, 32, 90],
    [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],
    [66, 88, 98, 47],
    [77, 65, 51, 77, 19, 15, 35, 19, 23]
  ]
  
  def padding(data):
    max_len = len(max(data, key=len))
    print(f"Maximum sequence length: {max_len}")
  
    for i, seq in enumerate(tqdm(data)):
      if len(seq) < max_len:
        data[i] = seq + [pad_id] * (max_len - len(seq))
  
    return data, max_len
    
  data, max_len = padding(data)
  ```

  같은 방식으로 패딩 전처리를 해준다.

  ```
  d_model = 8  # model의 hidden size
  num_heads = 2  # head의 개수
  inf = 1e12
  
  embedding = nn.Embedding(vocab_size, d_model)
  
  # B: batch size, L: maximum sequence length
  batch = torch.LongTensor(data)  # (B, L)
  batch_emb = embedding(batch)  # (B, L, d_model)
  ```

  이번에는 d_model의 크기를 8, num_heads를 2로 설정해두어 각 단어마다 2개의 Q,K,V의 vector를 구한다.

  ```
  padding_mask = (batch != pad_id).unsqueeze(1)  # (B, 1, L)
  
  print(padding_mask)
  print(padding_mask.shape)
  
  >>
  
  tensor([[[ True,  True,  True,  True,  True,  True,  True,  True, False, False]],
  
          [[ True,  True,  True,  True,  True, False, False, False, False, False]],
  
          [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],
  
          [[ True,  True,  True,  True, False, False, False, False, False, False]],
  
          [[ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])
  torch.Size([5, 1, 10])
  ```

  0으로 채워진 padding된 부분을 전부 False로 바꿔주었다. 

  ```
  nopeak_mask = torch.ones([1, max_len, max_len], dtype=torch.bool)  # (1, L, L)
  nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L)
  
  print(nopeak_mask)
  print(nopeak_mask.shape)]
  
  >>
  
  tensor([[[ True, False, False, False, False, False, False, False, False, False],
           [ True,  True, False, False, False, False, False, False, False, False],
           [ True,  True,  True, False, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False],
           [ True,  True,  True,  True,  True, False, False, False, False, False],
           [ True,  True,  True,  True,  True,  True, False, False, False, False],
           [ True,  True,  True,  True,  True,  True,  True, False, False, False],
           [ True,  True,  True,  True,  True,  True,  True,  True, False, False],
           [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],
           [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])
  torch.Size([1, 10, 10])
  ```

  다음으로 현재 단어 기준 미래의 단어들을 전부 torch.tril함수를 이용해 masking 처리를 해주었다.

  ```
  mask = padding_mask & nopeak_mask  # (B, L, L)
  
  print(mask)
  print(mask.shape)
  
  >>
  
  tensor([[[ True, False, False, False, False, False, False, False, False, False],
           [ True,  True, False, False, False, False, False, False, False, False],
           [ True,  True,  True, False, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False],
           [ True,  True,  True,  True,  True, False, False, False, False, False],
           [ True,  True,  True,  True,  True,  True, False, False, False, False],
           [ True,  True,  True,  True,  True,  True,  True, False, False, False],
           [ True,  True,  True,  True,  True,  True,  True,  True, False, False],
           [ True,  True,  True,  True,  True,  True,  True,  True, False, False],
           [ True,  True,  True,  True,  True,  True,  True,  True, False, False]],
  
          [[ True, False, False, False, False, False, False, False, False, False],
           [ True,  True, False, False, False, False, False, False, False, False],
           [ True,  True,  True, False, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False],
           [ True,  True,  True,  True,  True, False, False, False, False, False],
           [ True,  True,  True,  True,  True, False, False, False, False, False],
           [ True,  True,  True,  True,  True, False, False, False, False, False],
           [ True,  True,  True,  True,  True, False, False, False, False, False],
           [ True,  True,  True,  True,  True, False, False, False, False, False],
           [ True,  True,  True,  True,  True, False, False, False, False, False]],
  
          [[ True, False, False, False, False, False, False, False, False, False],
           [ True,  True, False, False, False, False, False, False, False, False],
           [ True,  True,  True, False, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False],
           [ True,  True,  True,  True,  True, False, False, False, False, False],
           [ True,  True,  True,  True,  True,  True, False, False, False, False],
           [ True,  True,  True,  True,  True,  True,  True, False, False, False],
           [ True,  True,  True,  True,  True,  True,  True,  True, False, False],
           [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],
           [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],
  
          [[ True, False, False, False, False, False, False, False, False, False],
           [ True,  True, False, False, False, False, False, False, False, False],
           [ True,  True,  True, False, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False]],
  
          [[ True, False, False, False, False, False, False, False, False, False],
           [ True,  True, False, False, False, False, False, False, False, False],
           [ True,  True,  True, False, False, False, False, False, False, False],
           [ True,  True,  True,  True, False, False, False, False, False, False],
           [ True,  True,  True,  True,  True, False, False, False, False, False],
           [ True,  True,  True,  True,  True,  True, False, False, False, False],
           [ True,  True,  True,  True,  True,  True,  True, False, False, False],
           [ True,  True,  True,  True,  True,  True,  True,  True, False, False],
           [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],
           [ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])
  torch.Size([5, 10, 10])
  ```

  padding된 부분을 masking한 값과 미래의 단어를 masking한 값을 &연산자를 이용해 최종적인 mask 형태의 값을 구했다.

  ```
  w_q = nn.Linear(d_model, d_model)
  w_k = nn.Linear(d_model, d_model)
  w_v = nn.Linear(d_model, d_model)
  
  w_0 = nn.Linear(d_model, d_model)
  
  q = w_q(batch_emb)  # (B, L, d_model)
  k = w_k(batch_emb)  # (B, L, d_model)
  v = w_v(batch_emb)  # (B, L, d_model)
  
  batch_size = q.shape[0]
  d_k = d_model // num_heads
  
  q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
  k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
  v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
  
  q = q.transpose(1, 2)  # (B, num_heads, L, d_k)
  k = k.transpose(1, 2)  # (B, num_heads, L, d_k)
  v = v.transpose(1, 2)  # (B, num_heads, L, d_k)
  
  print(q.shape)
  print(k.shape)
  print(v.shape)
  ```

  이전과 같이 Q,K,V vector를 구하였다.

  ```
  attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)
  
  masks = mask.unsqueeze(1)  # (B, 1, L, L)
  masked_attn_scores = attn_scores.masked_fill_(masks == False, -1 * inf)  # (B, num_heads, L, L)
  ```

  encoding vector를 구하기 위한 수식을 코드로 구현한 뒤에 softmax에 들어가기 전 해당 vector에 mask를 씌웠다.

  ```
  attn_dists = F.softmax(masked_attn_scores, dim=-1)  # (B, num_heads, L, L)
  attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)
  
  print(attn_values.shape)
  
  >>
  torch.Size([5, 2, 10, 4])
  
  attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)
  attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)
  
  print(attn_values.shape)
  
  >> 
  
  torch.Size([5, 10, 8])
  
  outputs = w_0(attn_values)
  ```

  그 이후 softmax 함수를 거쳐 V vector를 곱해준뒤 W_0를 곱하여 최종적인 encoding vector를 구하였다.

  ```
  class MultiheadAttention(nn.Module):
    def __init__(self):
      super(MultiheadAttention, self).__init__()
  
      # Q, K, V learnable matrices
      self.w_q = nn.Linear(d_model, d_model)
      self.w_k = nn.Linear(d_model, d_model)
      self.w_v = nn.Linear(d_model, d_model)
  
      # Linear transformation for concatenated outputs
      self.w_0 = nn.Linear(d_model, d_model)
  
    def forward(self, q, k, v, mask=None):
      batch_size = q.shape[0]
  
      q = self.w_q(q)  # (B, L, d_model)
      k = self.w_k(k)  # (B, L, d_model)
      v = self.w_v(v)  # (B, L, d_model)
  
      q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
      k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
      v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)
  
      q = q.transpose(1, 2)  # (B, num_heads, L, d_k)
      k = k.transpose(1, 2)  # (B, num_heads, L, d_k)
      v = v.transpose(1, 2)  # (B, num_heads, L, d_k)
  
      attn_values = self.self_attention(q, k, v, mask=mask)  # (B, num_heads, L, d_k)
      attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)
  
      return self.w_0(attn_values)
  
    def self_attention(self, q, k, v, mask=None):
      attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)
  
      if mask is not None:
        mask = mask.unsqueeze(1)  # (B, 1, L, L) or  (B, 1, 1, L)
        attn_scores = attn_scores.masked_fill_(mask == False, -1*inf)
  
      attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)
  
      attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)
  
      return attn_values
  ```

  ```
  multihead_attn = MultiheadAttention()
  
  outputs = multihead_attn(batch_emb, batch_emb, batch_emb, mask=mask)  # (B, L, d_model)
  ```

  전체적인 코드이다. mask가 존재하면 decoder에서 사용하고 mask가 없으면 encoder에서 사용할 수 있는 범용적인 코드이다.



- Encoder-Decoder attention

  ```
  trg_data = [
    [33, 11, 49, 10],
    [88, 34, 5, 29, 99, 45, 11, 25],
    [67, 25, 15, 90, 54, 4, 92, 10, 46, 20, 88 ,19],
    [16, 58, 91, 47, 12, 5, 8],
    [71, 63, 62, 7, 9, 11, 55, 91, 32, 48]
  ]
  
  trg_data, trg_max_len = padding(trg_data)
  ```

  ```
  # S_L: source maximum sequence length, T_L: target maximum sequence length
  src_batch = batch  # (B, S_L)
  trg_batch = torch.LongTensor(trg_data)  # (B, T_L)
  
  print(src_batch.shape)
  print(trg_batch.shape)
  
  >>
  
  torch.Size([5, 10])
  torch.Size([5, 12])
  ```

  trg_data는 디코더에 있는 단어들이다. 디코더에서 인코더와 마찬가지로 padding처리를 해주었다. 인코더에 입력된 단어의 수는 10개 디코더에 입력된 단어의 수는 12개로 단어의 수가 다르다.

  (ex, I want to go home -> 나는 집에 가기를 원한다. => 인코더: 5개 디코더: 4개)

  ```
  q = w_q(trg_emb)  # (B, T_L, d_model)
  k = w_k(src_emb)  # (B, S_L, d_model)
  v = w_v(src_emb)  # (B, S_L, d_model)
  
  batch_size = q.shape[0]
  d_k = d_model // num_heads
  
  q = q.view(batch_size, -1, num_heads, d_k)  # (B, T_L, num_heads, d_k)
  k = k.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)
  v = v.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)
  
  q = q.transpose(1, 2)  # (B, num_heads, T_L, d_k)
  k = k.transpose(1, 2)  # (B, num_heads, S_L, d_k)
  v = v.transpose(1, 2)  # (B, num_heads, S_L, d_k)
  
  print(q.shape)
  print(k.shape)
  print(v.shape)
  ```

  디코더에서 Multi-Head Attention을 거치고 난 뒤 Q vector와 인코더에서 입력받은 K,V vector를 Attention연산을 하는 과정이다.

  ```
  attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, T_L, S_L)
  attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, T_L, S_L)
  
  attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, T_L, d_k)
  ```

  그 이후 인코더의 단어와 가장 연관있는 디코더의 단어를 출력하게 된다.

  

