# Seq2Seq

<a href="https://ibb.co/dQrTpHD"><img src="https://i.ibb.co/19LgX4T/2021-02-17-22-37-19.png" alt="2021-02-17-22-37-19" border="0"></a>

위의 예시는 Seq2Seq를 이용해 Are you free tomorrow?라는 질문을 했을 때 Yes, what's up 이라고 답변하는 챗봇의 예시이다. RNN모델로 LSTM을 사용했다. RNN기반의 모델 구조 이기 때문에 hidden state의 dim이 고정된 상태로 계속해서 정보를 누적하여 짧은 문장에서는 높은 성능을 보여주지만 길이가 긴 문장에서는 앞단의 정보들이 약해지기 때문에 성능이 저하될 수 있다.

<a href="https://ibb.co/1GBjrzR"><img src="https://i.ibb.co/4RC3814/2021-02-17-22-38-33.png" alt="2021-02-17-22-38-33" border="0"></a>

Seq2Seq모델은 RNN구조에서 Many to Many에 해당된다.

# Seq2Seq with Attention

- Seq2Seq의 한계

  - Encoder의 출력인 context vector가 고정된 길이의 벡터이기 때문에 입력 문장의 길이와 관계없이 항상 같은 길이의 벡터로 변환한다. 가령, 엄청나게 긴 문장을 고정 길이의 벡터로 맞추다 보게 되면 한계가 생겨 필요한 정보들을 encoder의 출력이 못담을 수 있다. 따라서 이 문제를 해결하기 위해 Attention을 사용하게 된다. 

    <a href="https://imgbb.com/"><img src="https://i.ibb.co/rxjy8zD/2021-02-17-23-42-43.png" alt="2021-02-17-23-42-43" border="0"></a>

    

- Seq2Seq with Attention

  기존 Seq2Seq가 마지막 hidden state만을 decoder에 전달을 했다면 Attention 구조는 현재 입력에 대한 hidden state를 모두 이용하여 decoder로 전달을 하게 된다.  decoder의 hidden state와 encoder의 각 time step의 hidden state유사도를 구하여 softmax를 취해 각 hidden state의 비율을 알 수 있는데 이러한 벡터를 attention vector라 한다.(위의 그림에서 Attention distribution vector)

  

  .<a href="https://imgbb.com/"><img src="https://i.ibb.co/SKh1CjG/2021-02-17-23-46-38.png" alt="2021-02-17-23-46-38" border="0"></a>

  

  Attention score를 구하는 방식은 3가지로 나뉘게 된다. ( ht: decoder hidden state , hs: encoder에서의 각 step의 hidden state )

  1. decoder의 hidden state와 encoder의 각 time step 의 hidden state를 내적하는 방법.

  2. 두 hidden state 사이에 가중치행렬 W를 추가하는 방법

  3. 두 hidden state를 concat하여 가중치행렬W1을 곱해준 뒤에 활성함수 tanh를 거쳐 W2를 곱해주는 방법

     <a href="https://imgbb.com/"><img src="https://i.ibb.co/V9nxvd9/2021-02-17-23-52-16.png" alt="2021-02-17-23-52-16" border="0"></a>

     

- Attention Examples in Machine Translation 

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/jkdw02R/2021-02-17-23-57-22.png" alt="2021-02-17-23-57-22" border="0"></a>

  위의 그림처럼 디코더가 예측할 때 인코더 상의 어떤 단어에 집중했는지에 대한 시각화를 볼 수 있어서 파악하는데 용이하다.



# Greedy decoding

- Greedy decoding이란?
  - 일반적인 seq2seq with attention에서는 현재 time step에서 가장 높은 확률을 가지는 단어 하나를 생성 하는 Greedy decoding이다. 이러한 Greedy decoding 방법은 이전에 생성한 결과를 바꿀 수 없다.
- 한계
  - 단어 레벨에서 가장 좋은 것을 찾는 것이 올바른 문장을 만든다 보장 할 수 없다

# Exhaustive search

- Exhaustive search이란?
  - 가능한 모든 문장을 생성해서 제일 확률 높은 문장 고른다.
- 한계
  - 굉장히 시간이 오래걸린다.



# Beam search

- Beam search란?

  - beam search는 디코더의 각 time step마다 k개의 가능한 경우를 고려해 최종 k(beam size) 개의 output중에서 가장 확률이 높은것을 선택하는 방식이다. (보통 beam size는 5 ~ 10)

  - k개의 출력은 hypothesis라고 한다. 각 확률을 곱하는것으로 각 hypothesis의 확률을 구해야하지만 log를 사용해 더하는것으로 계산할 수 있다.

  - <a href="https://imgbb.com/"><img src="https://i.ibb.co/sHvt9Bc/2021-02-18-00-12-13.png" alt="2021-02-18-00-12-13" border="0"></a>

    <a href="https://ibb.co/XxC6JkL"><img src="https://i.ibb.co/VYqbBp9/2021-02-18-00-12-58.png" alt="2021-02-18-00-12-58" border="0"></a>

    - 가장 높은 확률을 가지는 k개를 계속해서 업데이트 해서 가져간다. 위의 예에서 처음 문장을 생성할 때 가장 확률이 높은 2개 he, i가 선택되고 다음 time step에서는 he, i에서 각각 가장 확률이 높은 2개씩을 선택해 확률을 계산하고 가장 확률이 높은 2개를 사용한다.
    - beam search decoding에서는 각 hypothesis가 다른 timestep에 <END>로 문장을 끝낼 수 있다 이런 경우 임시 저장해놓고 위의 과정을 계속 반복한다. (completed hypothesis)
    - beam search decoding 종료시기
      - 미리 정해놓은 time step T까지 도달할 때 종료
      - 저장해놓은 완성본이 n개 이상이 되었을 때 종료
    - completed hypothesis중에서 선택을 해야하는데 길이가 짧을 수록 확률이 높게 나오기 떄문에 Normalize를 해주어야한다.



# BLEU score

<a href="https://ibb.co/db0XVFz"><img src="https://i.ibb.co/jVvqQm1/2021-02-18-00-15-30.png" alt="2021-02-18-00-15-30" border="0"></a>

만약 predicted가 극단적으로 of Half my heart in is na ooh na 라고 출력되었을 때 정확도가 100%가 나오는 경우가 생기게 된다. 이를 해결하고자 BLEU Score를 사용하게 된다..

<a href="https://imgbb.com/"><img src="https://i.ibb.co/JFpwGJX/2021-02-18-00-18-27.png" alt="2021-02-18-00-18-27" border="0"></a>

기본 개념은 N-gram을 사용하게 된다.

<a href="https://ibb.co/dWL8C4N"><img src="https://i.ibb.co/gjPkqZ0/2021-02-18-00-18-54.png" alt="2021-02-18-00-18-54" border="0"></a>

1~4로 나누어 순서까지 정확한지에 대해 확인하는 방법이다.