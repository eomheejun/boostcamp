# GPT-1

- GPT-1

  <a href="https://ibb.co/TWqR2nv"><img src="https://i.ibb.co/LNh6Rqk/2021-02-19-20-16-34.png" alt="2021-02-19-20-16-34" border="0"></a>

  - Special token을 이용해 다양한  task를 처리할 수 있다.
  - 입력과 출력 sequence가 별도로 있는 것이 아니라, 순차적으로 생성을 하여 language modeling을 진행한다.
  - Extract라는 token은 문장을 잘 이해하고 논리적으로 내포,모순 관계를 예측하는데에 필요한 정보를 query로 attention에 사용된다.
  - 마지막 layer는 학습이 충분히 되어야 하지만, 이전의 layer에서는 learning rate을 상대적으로 작게 줌으로써 큰 변화가 이루어지지 않게 하여, pre-train model에는 큰 변화가 이루어지지 않도록 해준다.

  

# BERT

- Bert 

  <a href="https://ibb.co/jf6dRNr"><img src="https://i.ibb.co/3drwz3C/2021-02-19-20-21-24.png" alt="2021-02-19-20-21-24" border="0"></a>

  - Transformer 구조를 활용한 Language Representation에 관한 내용이다.
  - 기본적으로 대용량의 Unlabeled data로 모델을 미리 학습시키고 특정 Task를 가지고 있는 labeled data를 통해 전이 학습을 진행하는 모델을 의미한다.
  - 이전의 ELMo에서는 대용량의 unlabeled corpus를 통해 모델을 학습하고 이를 토대로 뒤 쪽에 특정 task를 처리하는 network를 붙이는 방식을 사용했다.
  - BERT는 이러한 과정 필요 없이 모델 자체의 Fine Tuning을 통해 해당 Task를 처리할 수 있다.

- 방법론

  - 기존 방법론에서는 일반적으로 전반부의 n개의 단어를 가지고 후반부의 단어를 예측하는 모델을 구성하였다. → Unidirectional
  - ELMo에서는 이를 해결하기 위해 **Bi-LSTM**을 사용했지만 이는 매우 shallow한 양방향성을 가질 수 밖에 없다
  - **Masked Language Model(MLM)**
    - Input에서 무작위하게 몇 개의 Token을 mask시키고 이를 Transformer의 인코더를 사용하여에 넣어 주변 단어의 context를 보고 mask된 단어를 예측하는 모델이다. 
    - Mask가 많으면 Context를 통해 값을 예측할 수 없다
    - Mask가 적으면 학습하는 비용이 늘어난다.
    - 이 때 Fine Tuning과정에서 Mask 토큰이 없는 상황이 일어나게되는데 이 때 Mask 되어야할 단어를 특정 비율로 나눠 Mask, 아예 다른 단어, 같은 단어로 치환하는 과정을 거쳐 난이도를 높인다.
  - **Next Sentence Prediction(NSP)**
    - 두 문장을 pre-training 시에 같이 넣어 두 문장이 이어지는 문장인지 확인하는 것.

  - 사전학습된 BERT 모델을 통해 각자의 Task에 맞는 Labeled Data를 통해 학습을 진행하고 Fine Tuning 과정을 거친다.

    <a href="https://ibb.co/mTgybvc"><img src="https://i.ibb.co/SyGm5Q3/2021-02-19-20-27-31.png" alt="2021-02-19-20-27-31" border="0"></a>

# BERT vs GPT-1

- 트레이닝 데이터 크기
  - GPT : 800M 단어 vs BERT : 2500M 단어
- 학습동안 special token 학습
  - BERT는 사전 교육 중에 [SEP], [CLS] 및 문장 A/B 임베딩을 학습한다.
- 배치사이즈
  - GPT : 32000단어 vs BERT : 128000단어
- Task-specific fine-tuning
  - GPT는 모든 미세 조정 실험에 동일한 5e-5 학습률을 사용한다. BERT는 작업별 미세 조정 학습 속도를 선택한다.



결론 :  BERT가 GPT-1보다는 성능이 좋다.



# GPT-2

- GPT-2란?
  - GPT-1에서 발전된 모델
  - 40GB의 텍스트가 훈련되있다.
    - 데이터셋의 품질이 양호한지 확인하기 위해 상당한 노력을 기울임
  - 언어 모델은 매개 변수나 아키텍처를 수정하지 않고 제로샷 설정에서 다운스트림 작업을 수행할 수 있다.
- Datasets
  - 다양하고 거의 무제한에 가까운 텍스트의 출처는 웹 스크래치이다.
    - 그들은 소셜 미디어 플랫폼인 웹텍스트인 Reddit에서 모든 아웃바운드 링크를 스크랩했다.
  - 수사망 및 신문을 사용하여 링크에서 내용 추출
  - Preprocess 
    -  Byte pair encoding (BPE) 
    - 여러 개의 vocab 토큰에 걸쳐 단어 조각화 최소화

- Modification

  - Layer normalization은 각 하위 블록의 입력으로 이동되었다.

    <a href="https://imgbb.com/"><img src="https://i.ibb.co/Qpv0dDk/2021-02-19-20-49-48.png" alt="2021-02-19-20-49-48" border="0"></a>

  - 마지막 self-attention이후에 Layer normalization이 추가되었다.

  

GPT-2에서 더 많은 데이터로 기존보다 큰 배치사이즈를 통해서 GPT-3가 탄생되었다.



# ALBERT

- 모델의 경량화

- 임베딩 벡터 크기를 줄인다.

- Transformer layer의 파라미터를 모든 레이어에서 공유하여 파라미터 개수(메모리)를 줄이고, 그만큼 모델의 크기를 늘려 SOTA를 달성한다.

- Next sentence prediction loss 대신 sentence order prediction loss를 사용한다.

  <a href="https://ibb.co/bXB8mDk"><img src="https://i.ibb.co/sqwMRzZ/2021-02-19-21-14-16.png" alt="2021-02-19-21-14-16" border="0"></a>

  Attention에 들어가는 Vector가 크면 정보를 더 많이 담고 있는 대신 메모리를 많이 잡아먹고 모델의 크기가 커지는 단점이 있다. 이로 인해 차원의 양을 줄여서 모델 사이즈의 크기를 작게 한 다.





# ELECTRA

- 토큰 교체를 정확하게 분류하는 인코더의 효율적인 학습

  - 텍스트 인코더를 생성자가 아닌 판별자로 사전교육한다.

  <a href="https://ibb.co/vLN416R"><img src="https://i.ibb.co/44bRWn3/2021-02-19-21-17-55.png" alt="2021-02-19-21-17-55" border="0"></a>

- GAN과 유사하지만 GAN을 텍스트에 적용하기 어렵기 때문에 ELECTRA를 사용한다.

  