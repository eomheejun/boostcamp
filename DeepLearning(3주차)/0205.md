# Generative Models

- Learning a Generative Model

  <a href="https://ibb.co/vD7b67S"><img src="https://i.ibb.co/52NdgNP/2021-02-05-11-43-01.png" alt="2021-02-05-11-43-01" border="0"></a>

  - Sampling :  트레이닝 데이터에 존재하지 않는 강아지 사진을 만들어낸다.

  - Anomaly detection : 어떤 이미지가 주어졌을 때 강아지인지 아닌지 구별해내는 것

  - Feature learning : 이미지가 가진 특징들 추출(예를들어 강아지는 꼬리가 있고 코가 있고 귀가 두개고 등등)

  여기서 말하는 P(x)는 어떤 X란 입력이 들어갔을때 어떤 값이 출력될 수 있고 혹은 X를 샘플링할 수 있는 모델일 수 있다.

  

  <a href="https://ibb.co/550RY6b"><img src="https://i.ibb.co/K9PrNXT/2021-02-05-11-43-09.png" alt="2021-02-05-11-43-09" border="0"></a>

  

  <a href="https://ibb.co/PFC41JX"><img src="https://i.ibb.co/k5c0qzb/image.png" alt="image" border="0"></a>

  <a href="https://ibb.co/HhTtn9c"><img src="https://i.ibb.co/RjT3zJX/2021-02-05-11-51-54.png" alt="2021-02-05-11-51-54" border="0"></a>

  - 파라미터 수가 왜 2^n-1개인가?

    <a href="https://ibb.co/WknXRv0"><img src="https://i.ibb.co/y60CxFp/8.jpg" alt="8" border="0"></a>

    위 그림은 픽셀이 4개인 경우를 예시이다. 각 경우에 확률이 parameter가 되고, 하나의 경우는 (1-나머지들의 합) 으로 표현 가능하기 때문에 파라미터 수는 2^n-1이다.

  

  위의 두 예시 모두 파라미터 수가 너무 많다. 다시 말해 학습이 잘 되지 않을 수 있다.

  <a href="https://ibb.co/YDDjzDy"><img src="https://i.ibb.co/hXX8PXZ/2021-02-05-11-52-30.png" alt="2021-02-05-11-52-30" border="0"></a>

  따라서 파라미터 수를 줄이기 위해 각 픽셀들이 독립적이라 가정하게 된다. 그렇게 되면 파라미터 수가 n개로 줄게 된다. 그러나 파라미터 수가 너무 적어 학습을 할 수 없어 각 픽셀들의 독립과 의존 중간의 파라미터 수를 찾아야 한다.

- Conditional Independence

  <a href="https://ibb.co/GW934TH"><img src="https://i.ibb.co/Stc74rf/2021-02-05-11-54-52.png" alt="2021-02-05-11-54-52" border="0"></a>

  위와같은 수식으로 적당한 파라미터 값을 찾는다.

  <a href="https://ibb.co/FVpJp6n"><img src="https://i.ibb.co/74hVhvb/2021-02-05-11-55-51.png" alt="2021-02-05-11-55-51" border="0"></a>

  체인룰만 사용하게 되면 이전과 파라미터 수가 달라지지 않아 엄청난 파라미터 수를 가지게 된다.

  <a href="https://ibb.co/cJM8PSy"><img src="https://i.ibb.co/zsj21Tm/2021-02-05-11-56-05.png" alt="2021-02-05-11-56-05" border="0"></a>

  따라서 위와같이 현재의 픽셀은 바로 직전의 픽셀과 의존적이고 나머지완 독립적이라 가정하여 파라미터 수를 구하게 된다. 그렇게 되면 체인룰로 얻어질수 있는  conditional distribution(조건부확률)들의 곱의 모양이 바귀게 된다. 그렇게 되면 파라미터 수가 2n-1개가 필요하게 되고 파라미터 수를 적절하게 만들 수 있게 된다.



- Auto-regressive Model

  <a href="https://ibb.co/JjDF8rR"><img src="https://i.ibb.co/7tsNBrJ/2021-02-05-12-01-01.png" alt="2021-02-05-12-01-01" border="0"></a>

  Auto-regressive Model은 현재 픽셀의 직전픽셀만 의존적인 것을 의미하지 않고 시작부터 현재까지의 모든 픽셀과 의존적인 것도 의미한다. 이전 1개만 고려를하게 되면 AR 1모델이라 부르고 N개만 고려를 하게 되면 AR N모델이라 부른다.

  

  <a href="https://ibb.co/5kKFmXw"><img src="https://i.ibb.co/qCJ5KLV/2021-02-05-12-01-22.png" alt="2021-02-05-12-01-22" border="0"></a>

  i번째 픽셀은 i-1번째 픽셀에 의존적이게 된다. NN입장에선 입력에서 차원이 달라지게 된다. 

  

  <a href="https://ibb.co/SwrJDGj"><img src="https://i.ibb.co/yFSVm79/2021-02-05-12-03-55.png" alt="2021-02-05-12-03-55" border="0"></a>

  

  

  <a href="https://ibb.co/VvydbJ0"><img src="https://i.ibb.co/P5pRJFs/2021-02-05-12-12-43.png" alt="2021-02-05-12-12-43" border="0"></a>

  앞에서 만든 모델은 Fully connected layer를 통해서 만들었다 i번째 픽셀은 i-1번째 픽셀만 의존적이나 그 이전에 i-1은 i-2, i-2는 i-3 계속해서 이어나가 첫번째부터 i-1번째 픽셀까지 모두 고려해 i번째 픽셀을 만든셈이었다. 그러나 Pixec RNN은 RNN을 통해 픽셀을 만들게 된다.





- Latent Variable Models

  - Variational Auto-encoder(VAE)

    <a href="https://ibb.co/xfJSJFX"><img src="https://i.ibb.co/BrqVqnc/2021-02-05-12-20-29.png" alt="2021-02-05-12-20-29" border="0"></a>

    posterior distribution : p(x|z), 사건이 발생한 후(관측이 진행된 후) 그 사건이 특정 모델에서 발생했을 확률분포

    variational distribution : posterior distribution을 구할 수 없는 경우가 많아. 따라서 posterior에 근사하는 분포를 만든다.

    

    어떻게 posterior를 모르는데 근사를 할까?

    <a href="https://ibb.co/LtgZc7r"><img src="https://i.ibb.co/QNHKTwk/2021-02-05-12-26-00.png" alt="2021-02-05-12-26-00" border="0"></a>

    ELBO를 Maximize하게되면 posterior를 몰라도 variational distribution을 구할 수 있다.

    <a href="https://ibb.co/W5yzPDz"><img src="https://i.ibb.co/dM2pLbp/2021-02-05-12-28-19.png" alt="2021-02-05-12-28-19" border="0"></a>

    <a href="https://ibb.co/0Q3zdG3"><img src="https://i.ibb.co/3kHPLCH/2021-02-05-12-35-17.png" alt="2021-02-05-12-35-17" border="0"></a>

    ELBO의 수식을 치환하게 되면 <a href="https://imgbb.com/"><img src="https://i.ibb.co/DpQxXt9/2021-02-05-12-34-46.png" alt="2021-02-05-12-34-46" border="0"></a>는 위와 같은 식이 되게 된다. ELBO의 값을 최대화 하는 방법으로 variational distribution을 구하므로 위 식에서 Reconstruction term은 1에 가깝게 Regularization term은 가장 작게 만들어 주면 된다.



- Generative Adversarial Network(GAN)

  <a href="https://ibb.co/vBnvNmk"><img src="https://i.ibb.co/7zmN3VK/2021-02-05-12-40-39.png" alt="2021-02-05-12-40-39" border="0"></a>

  <a href="https://ibb.co/hVFxdf0"><img src="https://i.ibb.co/tDpw2XR/2021-02-05-12-48-16.png" alt="2021-02-05-12-48-16" border="0"></a>

  x라는 이미지가 주어졌을 때 원본데이터 D(discriminator,판별자)는 log(D(x))의 기댓갑을(이 이미지가 존재하는 이미지면 1 아니면 0) 1로 분류될 수있도록 학습하고 G(generator,생성자)는 이 이미지가 판별자에 의해 진짜로 인식될수 있도록 학습을 진행한다.

  - 기댓값

    <a href="https://ibb.co/sFb9MHP"><img src="https://i.ibb.co/wKBhxsL/2021-02-05-12-53-12.png" alt="2021-02-05-12-53-12" border="0"></a>

    <a href="https://ibb.co/FnhLcMH"><img src="https://i.ibb.co/cxySRHY/2021-02-05-12-54-18.png" alt="2021-02-05-12-54-18" border="0"></a>

    코드상에서 미니배치 마다 D먼저 혹은 G먼저 번갈아가며 학습을 하게 된다.

    <a href="https://ibb.co/3shtzHv"><img src="https://i.ibb.co/26KXNBS/2021-02-05-12-55-01.png" alt="2021-02-05-12-55-01" border="0"></a>

    GAN에서의 목표는 Pg라는 가짜 이미지가 Pdata라는 원본이미지의 정규분포를 잘 따라가도록 하는 것과 가짜이미지를 G로 생성한 뒤 D로 판별했을 때 값이 1/2로 수렴하는 것이다.

    <a href="https://ibb.co/DrgYnSR"><img src="https://i.ibb.co/fDG97Px/2021-02-05-12-56-41.png" alt="2021-02-05-12-56-41" border="0"></a>

    판별자D는 위와 같은 값에서 극댓값을 가지게 된다. 

    <a href="https://ibb.co/nknNztk"><img src="https://i.ibb.co/tCxT8FC/2021-02-05-12-57-51.png" alt="2021-02-05-12-57-51" border="0"></a>

    따라서  Pg라는 가짜 이미지가 Pdata라는 원본이미지의 분포와 동일하게 된다면 극댓값을 가지는 지점은 1/2로 수렴하게 되는 것이다.  Pg와 Pdata 는 같은 값을 가지기 때문에 JSD값은 0이되고 우리는 -log(4) 라는 값을 가지게 된다. 

  

  

  

