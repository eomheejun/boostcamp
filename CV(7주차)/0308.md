# VGG11 Fine Tuning의 장점

```
# Seed
import torch
import numpy as np
import random

torch.manual_seed(0)
torch.cuda.manual_seed(0)
np.random.seed(0)
random.seed(0)

# Ignore warnings
import warnings
warnings.filterwarnings('ignore')
```

```
import torch
import torch.nn as nn

class VGG11(nn.Module):
  def __init__(self, num_classes=1000):
    super(VGG11, self).__init__()

    self.relu = nn.ReLU(inplace=True)
    
    # Convolution Feature Extraction Part
    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
    self.bn1   = nn.BatchNorm2d(64)
    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

    '''==========================================================='''
    '''======================== TO DO (1) ========================'''
    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
    self.bn2   = nn.BatchNorm2d(128)
    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

    self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
    self.bn3_1   = nn.BatchNorm2d(256)
    self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
    self.bn3_2   = nn.BatchNorm2d(256)
    self.pool3   = nn.MaxPool2d(kernel_size=2, stride=2)

    self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
    self.bn4_1   = nn.BatchNorm2d(512)
    self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.bn4_2   = nn.BatchNorm2d(512)
    self.pool4   = nn.MaxPool2d(kernel_size=2, stride=2)

    self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.bn5_1   = nn.BatchNorm2d(512)
    self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)
    self.bn5_2   = nn.BatchNorm2d(512)
    self.pool5   = nn.MaxPool2d(kernel_size=2, stride=2)
    '''======================== TO DO (1) ========================'''
    '''==========================================================='''
    

    # Fully Connected Classifier Part
    self.fc1      = nn.Linear(512 * 7 * 7, 4096)
    self.dropout1 = nn.Dropout(0.5)
    
    '''==========================================================='''
    '''======================== TO DO (2) ========================'''
    self.fc2      = nn.Linear(4096, 4096)
    self.dropout2 = nn.Dropout(0.5)
    
    self.fc3      = nn.Linear(4096, 1000)
    '''======================== TO DO (2) ========================'''
    '''==========================================================='''
    

  def forward(self, x):
    # Convolution Feature Extraction Part
    x = self.conv1(x)
    x = self.bn1(x)
    x = self.relu(x)
    x = self.pool1(x)

    x = self.conv2(x)
    x = self.bn2(x)
    x = self.relu(x)
    x = self.pool2(x)

    x = self.conv3_1(x)
    x = self.bn3_1(x)
    x = self.relu(x)
    x = self.conv3_2(x)
    x = self.bn3_2(x)
    x = self.relu(x)
    x = self.pool3(x)

    x = self.conv4_1(x)
    x = self.bn4_1(x)
    x = self.relu(x)
    x = self.conv4_2(x)
    x = self.bn4_2(x)
    x = self.relu(x)
    x = self.pool4(x)

    x = self.conv5_1(x)
    x = self.bn5_1(x)
    x = self.relu(x)
    x = self.conv5_2(x)
    x = self.bn5_2(x)
    x = self.relu(x)
    x = self.pool5(x)

    # Fully Connected Classifier Part
    x = torch.flatten(x, 1)
    x = self.fc1(x)
    x = self.relu(x)
    x = self.dropout1(x)
    
    x = self.fc2(x)
    x = self.relu(x)
    x = self.dropout2(x)
    
    x = self.fc3(x)
    return x
```

<a href="https://ibb.co/xMdDrmq"><img src="https://i.ibb.co/THdkNKM/2021-03-08-23-18-15.png" alt="2021-03-08-23-18-15" border="0"></a>

위의 VGG11은 논문을 참조하여 만들었다.

```
# Network
model = VGG11(num_classes=1000)

# Random input
x = torch.randn((1, 3, 224, 224))

# Forward
out = model(x)

# Check the output shape
print("Output tensor shape is :", out.shape)

>>

Output tensor shape is : torch.Size([1, 1000])

```

```
# Dataset
import torch
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader

import os
import cv2
import numpy as np
from glob import glob

class MaskDataset(Dataset):
  def __init__(self, data_root, is_Train=True, input_size=224, transform=None):
    super(MaskDataset, self).__init__()

    self.img_list = self._load_img_list(data_root, is_Train)
    self.len = len(self.img_list)
    self.input_size = input_size
    self.transform = transform

  def __getitem__(self, index):
    img_path = self.img_list[index]
    
    # Image Loading
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img/255.

    if self.transform:
      img = self.transform(img)

    # Ground Truth
    label = self._get_class_idx_from_img_name(img_path)

    return img, label

  def __len__(self):
    return self.len

  def _load_img_list(self, data_root, is_Train):
    # Change the name of directory which has inconsistent naming rule.
    full_img_list = glob(data_root + '/*')
    for dir in full_img_list:
      dirname = os.path.basename(dir)
      if '-1' in dirname:
        os.rename(dir, dir.replace(dirname, dirname.replace('-1', '1')))
    
    # ID < 1000 for Training (N=721)
    # 1000 < ID < 1050 for Validation (N=63)
    img_list = []
    for dir in glob(data_root + '/*'):
      if is_Train and (self._load_img_ID(dir) < 500):
        img_list.extend(glob(dir+'/*'))
      elif not is_Train and (1000 < self._load_img_ID(dir) < 1050):
        img_list.extend(glob(dir+'/*'))

    return img_list

  def _load_img_ID(self, img_path):
    return int(os.path.basename(img_path).split('_')[0])

  def _get_class_idx_from_img_name(self, img_path):
    img_name = os.path.basename(img_path)

    if 'normal' in img_name: return 0
    elif 'mask1' in img_name: return 1
    elif 'mask2' in img_name: return 2
    elif 'mask3' in img_name: return 3
    elif 'mask4' in img_name: return 4
    elif 'mask5' in img_name: return 5
    elif 'incorrect_mask' in img_name: return 6
    else:
      raise ValueError("%s is not a valid filename. Please change the name of %s." % (img_name, img_path))
```

```
# Dataset and Data Loader
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((224,224)),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                          std=[0.229, 0.224, 0.225])
])

train_dataset = MaskDataset(data_root, is_Train=True, input_size=input_size, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True)

valid_dataset = MaskDataset(data_root, is_Train=False, input_size=input_size, transform=transform)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, pin_memory=True, shuffle=False)
```

```
# Misc

class AverageMeter(object):
  """Computes and stores the average and current value"""
  def __init__(self):
      self.reset()

  def reset(self):
    self.val = 0
    self.avg = 0
    self.sum = 0
    self.count = 0

  def update(self, val, n=1):
    self.val = val
    self.sum += val * n
    self.count += n
    self.avg = self.sum / self.count
```

우선 dataset 을 만들고 pretrain되지 않은 모델로 학습을 시켜 본다.

```
from torchvision.models import vgg11

'''======================== TO DO Setting ========================'''
pretrained = False
'''==============================================================='''

model = vgg11(pretrained)
model.classifier[6] = nn.Linear(in_features=4096, out_features=7, bias=True)
model.cuda()
```

```
# Loss function and Optimizer
from torch.optim import Adam

criterion = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=lr)
```

```
# Main
os.makedirs(log_dir, exist_ok=True)

with open(os.path.join(log_dir, 'scratch_train_log.csv'), 'w') as log:
  # Training
  for iter, (img, label) in enumerate(train_loader):
    '''================================================================'''
    '''======================== TO DO Main (1) ========================'''
    # optimizer에 저장된 미분값을 0으로 초기화
    optimizer.zero_grad()
    

    # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)
    img, label = img.type(torch.FloatTensor).cuda(), label.cuda()

    # 모델에 이미지 forward
    pred_logit = model(img)

    # loss 값 계산
    loss = criterion(pred_logit,label)

    # Backpropagation
    loss.backward()
    optimizer.step
    

    # Accuracy 계산
    pred_label = (torch.argmax(pred_logit, 1) == label).sum()
    acc = pred_label.item() / len(label)
    '''======================== TO DO Main (1) ========================'''
    '''================================================================'''

    train_loss = loss.item()
    train_acc = acc

    # Validation for every 20 epoch
    if (iter % 20 == 0) or (iter == len(train_loader)-1):
      valid_loss, valid_acc = AverageMeter(), AverageMeter()

      for img, label in valid_loader:
        '''================================================================'''
        '''======================== TO DO Main (2) ========================'''
        # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)
        img, label = img.type(torch.FloatTensor).cuda(), label.cuda()

        # 모델에 이미지 forward (gradient 계산 X)
        with torch.no_grad():
          pred_logit = model(img)
        

        # loss 값 계산
        loss = criterion(pred_logit,label)

        # Accuracy 계산
        pred_label = (torch.argmax(pred_logit, 1) == label).sum()
        acc = pred_label.item() / len(label) 
        '''======================== TO DO Main (2) ========================'''
        '''================================================================'''

        valid_loss.update(loss.item(), len(img))
        valid_acc.update(acc, len(img))

      valid_loss = valid_loss.avg
      valid_acc = valid_acc.avg

      print("Iter [%3d/%3d] | Train Loss %.4f | Train Acc %.4f | Valid Loss %.4f | Valid Acc %.4f" %
            (iter, len(train_loader), train_loss, train_acc, valid_loss, valid_acc))
      
      # Train Log Writing
      log.write('%d,%.4f,%.4f,%.4f,%.4f\n'%(iter, train_loss, train_acc, valid_loss, valid_acc))
      
>>

Iter [  0/120] | Train Loss 1.9388 | Train Acc 0.0000 | Valid Loss 1.9496 | Valid Acc 0.1646
Iter [ 20/120] | Train Loss 1.9062 | Train Acc 0.5000 | Valid Loss 1.9402 | Valid Acc 0.1708
Iter [ 40/120] | Train Loss 1.9154 | Train Acc 0.2500 | Valid Loss 1.9525 | Valid Acc 0.1149
Iter [ 60/120] | Train Loss 1.9656 | Train Acc 0.1250 | Valid Loss 1.9480 | Valid Acc 0.1460
Iter [ 80/120] | Train Loss 1.9332 | Train Acc 0.0000 | Valid Loss 1.9466 | Valid Acc 0.1522
Iter [100/120] | Train Loss 2.0166 | Train Acc 0.1250 | Valid Loss 1.9471 | Valid Acc 0.1242
Iter [119/120] | Train Loss 2.0029 | Train Acc 0.0000 | Valid Loss 1.9477 | Valid Acc 0.1491
```

그 다음은 pretrained된 모델을 사용해 본다.

```
from torchvision.models import vgg11

'''======================== TO DO Setting ========================'''
pretrained = True
'''==============================================================='''

model = vgg11(pretrained)
model.classifier[6] = nn.Linear(in_features=4096, out_features=7, bias=True)
model.cuda()

# Freeze the feature extracting convolution layers
for param in model.features.parameters():
    param.requires_grad = False
```

```
# Loss function and Optimizer
from torch.optim import Adam

criterion = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=lr)
```

```
# Main
os.makedirs(log_dir, exist_ok=True)

with open(os.path.join(log_dir, 'fine_tuned_train_log.csv'), 'w') as log:
  # Training
  for iter, (img, label) in enumerate(train_loader):
    '''================================================================'''
    '''======================== TO DO Main (1) ========================'''
    # optimizer에 저장된 미분값을 0으로 초기화
    optimizer.zero_grad()

    # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)
    img, label = img.type(torch.FloatTensor).cuda(),label.cuda()

    # 모델에 이미지 forward
    pred_logit = model(img)

    # loss 값 계산
    loss = criterion(pred_logit,label)

    # Backpropagation
    loss.backward()
    optimizer.step()

    # Accuracy 계산
    pred_label = (torch.argmax(pred_logit,1) == label).sum()
    acc = pred_label.item() / len(label)
    '''======================== TO DO Main (1) ========================'''
    '''================================================================'''

    train_loss = loss.item()
    train_acc = acc

    # Validation for every 20 epoch
    if (iter % 20 == 0) or (iter == len(train_loader)-1):
      valid_loss, valid_acc = AverageMeter(), AverageMeter()

      for img, label in valid_loader:
        '''================================================================'''
        '''======================== TO DO Main (2) ========================'''
        # GPU 연산을 위해 이미지와 정답 tensor를 GPU로 보내기 (필요한 경우, 변수의 type도 수정해주세요)
        img, label = img.type(torch.FloatTensor).cuda() , label.cuda()

        # 모델에 이미지 forward (gradient 계산 X)
        with torch.no_grad():
          pred_logit = model(img)

        

        # loss 값 계산
        loss = criterion(pred_logit,label)

        # Accuracy 계산
        pred_label = (torch.argmax(pred_logit,1) == label).sum()
        acc = pred_label.item() / len(label)
        '''======================== TO DO Main (2) ========================'''
        '''================================================================'''

        valid_loss.update(loss.item(), len(img))
        valid_acc.update(acc, len(img))

      valid_loss = valid_loss.avg
      valid_acc = valid_acc.avg

      print("Iter [%3d/%3d] | Train Loss %.4f | Train Acc %.4f | Valid Loss %.4f | Valid Acc %.4f" %
            (iter, len(train_loader), train_loss, train_acc, valid_loss, valid_acc))
      
      # Train Log Writing
      log.write('%d,%.4f,%.4f,%.4f,%.4f\n'%(iter, train_loss, train_acc, valid_loss, valid_acc))
      

>>

Iter [  0/120] | Train Loss 2.0458 | Train Acc 0.0000 | Valid Loss 2.0253 | Valid Acc 0.2702
Iter [ 20/120] | Train Loss 1.5721 | Train Acc 0.5000 | Valid Loss 1.5522 | Valid Acc 0.3944
Iter [ 40/120] | Train Loss 1.3691 | Train Acc 0.5000 | Valid Loss 1.3968 | Valid Acc 0.4596
Iter [ 60/120] | Train Loss 0.2717 | Train Acc 0.8750 | Valid Loss 1.4634 | Valid Acc 0.4876
Iter [ 80/120] | Train Loss 0.9991 | Train Acc 0.5000 | Valid Loss 1.1772 | Valid Acc 0.5932
Iter [100/120] | Train Loss 0.7656 | Train Acc 0.7500 | Valid Loss 1.1379 | Valid Acc 0.5994
Iter [119/120] | Train Loss 0.6824 | Train Acc 0.7143 | Valid Loss 1.5888 | Valid Acc 0.5248
```

Loss값과 Accuracy를 pretrained된 모델과 되지 않은 모델을 비교해 봤을 때 성능차이가 많이 났다.