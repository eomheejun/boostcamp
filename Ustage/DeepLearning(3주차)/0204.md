# Recurrent Neural Networks(RNN)

- Sequential Model

  - Sequential 데이터를 처리할 때 어려운 점
    - 입력의 수가 정해져있지 않다.

  <a href="https://ibb.co/nMC9CD8"><img src="https://i.ibb.co/7t4P4GN/2021-02-04-16-07-40.png" alt="2021-02-04-16-07-40" border="0"></a>

  <a href="https://ibb.co/9G84FWT"><img src="https://i.ibb.co/DLVMxQC/2021-02-04-16-11-38.png" alt="2021-02-04-16-11-38" border="0"></a>

  중간의 hidden state를 넣어 과거의 정보를 summary한다.

  

- RNN

  <a href="https://ibb.co/Kqb0wgt"><img src="https://i.ibb.co/0MmDtL0/2021-02-04-16-13-04.png" alt="2021-02-04-16-13-04" border="0"></a>

  시간순으로 나열했을 때 입력값이 굉장히 많은 Fully connect layer로 표현이 가능하다.

  <a href="https://ibb.co/RbMRzW6"><img src="https://i.ibb.co/3d8gBHF/2021-02-04-16-16-10.png" alt="2021-02-04-16-16-10" border="0"></a>

  중첩되는 구조가 들어간다. phi를 sigmoid로 가정했을 때 h0에서 온 정보가 멀리가게 되면 계속해서 줄어들어 의미가 없어지게 된다. ReLU를 사용한다 가정했을 땐 엄청나게 큰값이 되어 네트워크가 폭발해버려 학습이 안되는 상황이 생긴다.

- Long Short Term Memory(LSTM)

  <a href="https://ibb.co/Wz1TT7L"><img src="https://i.ibb.co/hdvqqrS/2021-02-04-16-19-09.png" alt="2021-02-04-16-19-09" border="0"></a>

  <a href="https://ibb.co/TBYKV45"><img src="https://i.ibb.co/PZFT7c8/2021-02-04-16-18-06.png" alt="2021-02-04-16-18-06" border="0"></a>

  <a href="https://ibb.co/s57pxVd"><img src="https://i.ibb.co/8zknv7L/2021-02-04-16-20-27.png" alt="2021-02-04-16-20-27" border="0"></a>

  previous cell state : 이전의 정보들을 summary한다. 

  

  <a href="https://ibb.co/bsDBJ7J"><img src="https://i.ibb.co/CbD72W2/2021-02-04-16-22-25.png" alt="2021-02-04-16-22-25" border="0"></a>

  

  <a href="https://ibb.co/0V47KND"><img src="https://i.ibb.co/cXfBw5g/2021-02-04-16-23-42.png" alt="2021-02-04-16-23-42" border="0"></a>

  Forget Gate : 현재의 정보와 이전의 입력을 취합해 어떤 정보를 버릴지 결정한다

  Input Gate : 현재의 입력을 무작정 cell state에 올리지 않고 어떤 정보를 올릴지 **결정**한다

  

  <a href="https://ibb.co/HFt1MPn"><img src="https://i.ibb.co/Pmc3bWw/2021-02-04-16-23-46.png" alt="2021-02-04-16-23-46" border="0"></a>

  Update cell : Forget Gate 와 Input Gate 의 출력을 Combine하여 cell state에 올리게 된다.

  Output Gate : Input Gate에서 결정된값을 다음 Hidden state와 출력으로 내보낸다.



- GRU

  <a href="https://ibb.co/DwZ6Cbk"><img src="https://i.ibb.co/zPzLR5b/2021-02-04-16-30-03.png" alt="2021-02-04-16-30-03" border="0"></a>

  LSTM과 다르게 Gate가 2개다. Cell state가 없고 그 역할을 Hidden state가 겸한다.

   

- LSTM , GRU는  Transformer가 나오면서 많이 활용되고 있지 않다.





# Transformer

- Sequential data를 다루는 방법론

  <a href="https://ibb.co/3RNXjyQ"><img src="https://i.ibb.co/TW8X7Bd/2021-02-04-17-01-45.png" alt="2021-02-04-17-01-45" border="0"></a>

  RNN은 재귀적으로 들어가는 구조였다면 Transformer 는 attention이란 구조를 활용했다.

  

  <a href="https://ibb.co/syjvf6Q"><img src="https://i.ibb.co/SRNXjd0/2021-02-04-17-06-27.png" alt="2021-02-04-17-06-27" border="0"></a>

  Transformer는 Sequence데이터를  처리하고 데이터를 인코딩하는 방법이기 때문에 NMT(신경기계번역)문제에서만 적용되는 것이 아니라 이미지 분류, 이미지 detection 등 많은 곳에서 많이 사용된다.

  <a href="https://ibb.co/s5NXYps"><img src="https://i.ibb.co/cbZsnK2/2021-02-04-17-19-43.png" alt="2021-02-04-17-19-43" border="0"></a>

  <a href="https://ibb.co/P623fvJ"><img src="https://i.ibb.co/YL5YqFJ/2021-02-04-17-20-02.png" alt="2021-02-04-17-20-02" border="0"></a>

  RNN같은 경우의 3개의 단어가 입력으로 들어가게 되면 Neural Network가 3번 돌게 된다. 그러나 Transformer encoder는 재귀적으로 돌지 않는다. 





- Transformer에서 이해해야 될 point
  1. N개의 단어가 어떻게 encoder에서 한번에 처리가 되는지
  2. encoder와 decoder사이에 어떤 정보들을 주고 받는지
  3. decoder가 어떻게 Generation(단어를 하나씩 출력)할 수 있는지



- Point

  <a href="https://ibb.co/Sf2vqQH"><img src="https://i.ibb.co/XY9Jmy6/2021-02-04-17-26-35.png" alt="2021-02-04-17-26-35" border="0"></a>

  self-attention가 왜 Transformer 를 잘되게 하는지의 관건이다 Feed Foward Neural Network는 MLP와 동일하다.

  <a href="https://ibb.co/Gn9xSct"><img src="https://i.ibb.co/PFMmHw9/2021-02-04-18-36-06.png" alt="2021-02-04-18-36-06" border="0"></a>

  3개의 단어가 입력으로 들어온다 가정해보자. 각 단어마다 기계가 번역할 수 있게 특정 숫자로 벡터로 표현하게 된다.

  <a href="https://ibb.co/qJYqsbG"><img src="https://i.ibb.co/SfwYnqT/2021-02-04-18-37-02.png" alt="2021-02-04-18-37-02" border="0"></a>

  Self-attention에서 3개의 단어가 주어지면 3개의 벡터로 찾아준다. x1이 z1으로 넘어갈 때 x2,x3의 단어를 활용한다. 

  좀 더 단순한 예를 들어 2개의 단어가 주어졌다 가정해본다.

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/fryFnXF/2021-02-04-18-39-35.png" alt="2021-02-04-18-39-35" border="0"></a>

  2개의 단어를 각각 x1, x2벡터로 표현한다. 

  <a href="https://ibb.co/M54DnWG"><img src="https://i.ibb.co/mFwCJ1X/2021-02-04-18-41-38.png" alt="2021-02-04-18-41-38" border="0"></a>

  각 단어의 벡터들을 Neural network를 통해 q1,k1,v1벡터로 표현한다.

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/s2yh1T2/2021-02-04-18-43-15.png" alt="2021-02-04-18-43-15" border="0"></a>

  각 단어마다 score벡터란걸 만든다. score벡터는 인코딩 하고자하는 단어의 쿼리벡터와 나머지 모든 단어들의 키벡터를 내적한다. 이 쿼리 벡터와 나머지 모든단어들의 키벡터를 내적하는 의미는 내가 인코딩하고자하는 i번째 단어가 나머지 단어와 얼마나 관계가 있는지에 대해 알 수 있다. 

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/zrW1b3R/2021-02-04-18-49-05.png" alt="2021-02-04-18-49-05" border="0"></a>

  스코어 벡터가 나오게 되면 정규화해준다. 키 벡터의 차원을 나타내는 d값에 루트를씌워 score벡터에서 나눠주게 된다. 그 후 softmax함수를 적용시키게 된다. 이 값을 Attention weight라고 말한다.

  <a href="https://ibb.co/jhgdyHp"><img src="https://i.ibb.co/rk4gxvW/2021-02-04-18-58-52.png" alt="2021-02-04-18-58-52" border="0"></a>

  attention weight은 각각의 단어가 자기 자신과 나머지 단어들 얼마나 관계가 있는지에 대한 값이다. 이 scalar값을 다시 value 벡터에 곱해주게 되고 이 값은 해당 단어의 encoding 벡터라고 말한다. 

  <a href="https://ibb.co/d71Rcky"><img src="https://i.ibb.co/3kGnsSH/2021-02-04-19-03-40.png" alt="2021-02-04-19-03-40" border="0"></a>

  다시 그림으로 보게 되면 2x4 matrix(2개의 단어가 있고 각단어를 4차원으로 표현) 가있고 각 Q,K,V벡터를 찾아내는 MLP이 있다. 따라서 2개의 단어가 주어져있으므로 2개의 Q,K,VQ벡터가 나오게 된다.

  <a href="https://ibb.co/s9hNz6P"><img src="https://i.ibb.co/whTPjwL/2021-02-04-19-08-00.png" alt="2021-02-04-19-08-00" border="0"></a>

  그 후 Q벡터와 K벡터를 내적 후 루트 d로 나누어 softmax함수를 지나 그 scalar값을 V벡터와 곱한 값이 최종적으로 인코딩된 값이다.

  

  - 왜 잘될까?

    이미지 하나가 주어졌다 가정해 보자. 이미지 하나를 CNN이나 MLP로 차원을 바꾸게 될때 입력이 고정이면 Convolution filter나 Weight가 고정되어 있어 출력도 고정되어 나오게 된다. 그러나 Transformer는 입력이 고정되어 있다 하더라도 다른 입력들에 따라서 출력이 달라질 수 있기 때문에 그래서 훨씬 더 많은 것을 표현 할 수 있다.

  

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/P98myYj/2021-02-04-19-16-27.png" alt="2021-02-04-19-16-27" border="0"></a>

  하나의 임베딩된 벡터에 대해서 Q,K,V벡터를 여러개 만드는 것이 Multi-headed attention이라고 한다.

  <a href="https://ibb.co/sqdrX00"><img src="https://i.ibb.co/ZxpCbnn/2021-02-04-19-17-43.png" alt="2021-02-04-19-17-43" border="0"></a>

  위 처럼 8개의 인코딩된 벡터를 나오게 되면 

  

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/bJpzWLC/2021-02-04-19-19-08.png" alt="2021-02-04-19-19-08" border="0"></a>

  인코딩 벡터가 다음으로 넘어가기 위해 임베딩벡터와 차원이 같아야 한다. 따라서 8개의 인코딩벡터를 연결하게 되여 (8개의 인코딩벡터들의 차원,임베딩 벡터의 차원)matrix를 곱해주어 차원을 맞춰주게 된다.

  <a href="https://ibb.co/yXsCztb"><img src="https://i.ibb.co/R9HKXfV/2021-02-04-19-22-30.png" alt="2021-02-04-19-22-30" border="0"></a>

   

  <a href="https://ibb.co/5GL57Rg"><img src="https://i.ibb.co/LY1CGZX/2021-02-04-19-23-39.png" alt="2021-02-04-19-23-39" border="0"></a>

  그 후에 입력에 특정 값을 더해주는 positional encoding을 만들게 된다. 그 이유는 [a,b,c,d]라는 입력이 들어왔을 때 각 단어들의 인코딩된 값은 순서가 바뀌어도 다르지 않다. 따라서 들어온 입력값의 위치정보를 포함시키기 위하여 특정 bias를 더해 positional encoding값을 만들게 된다.

  <a href="https://ibb.co/099CDTr"><img src="https://i.ibb.co/R663vfc/2021-02-04-19-28-14.png" alt="2021-02-04-19-28-14" border="0"></a>

  

  <a href="https://ibb.co/B44Pd51"><img src="https://i.ibb.co/1ssnhtS/2021-02-04-19-30-09.png" alt="2021-02-04-19-30-09" border="0"></a>

  각 인코딩 된값들을 Layer normalization하게 된다. 

  

  - Layer normalization 이란? 

    <a href="https://ibb.co/4fWM2tT"><img src="https://i.ibb.co/thBczKs/2021-02-04-19-33-47.png" alt="2021-02-04-19-33-47" border="0"></a>

    BN은 batch 차원에서 정규화가 이루어 지고 LN은 Feature차원에서 정규화가 이루어 지게 된다. LN은 batch 크기와 상관없이 이루어 진다.

    

    <a href="https://imgbb.com/"><img src="https://i.ibb.co/pQgCdz9/2021-02-04-19-35-22.png" alt="2021-02-04-19-35-22" border="0"></a>

    인코더로 주어진 단어를 표현했고 디코더는 인코딩된 정보를 가지고 생성하는 작업이다.

    중요한 점은 인코더에서 디코더로 어떤 정보가 주어지는 지다.

    <a href="https://ibb.co/2vYHfm3"><img src="https://i.ibb.co/0CVvd8y/2021-02-04-19-38-15.png" alt="2021-02-04-19-38-15" border="0"></a>

    위에 그림과 같이 인코더에서 디코더로 K벡터와 V벡터를 전달하게 된다. 그 이유는 n번째 단어를 입력으로 받았을 때 n번째 단어의 Q벡터와 나머지 단어들의 K벡터를 곱해서 Attention을 만들고 V벡터를 weight sum한다. 그래서 입력에 있는 단어들을 디코더에있는 출력하고자 하는 단어들에 대해서 Attention map을 만들기 위해 K,V벡터를 전달하게 되는 것이다. 그 이후 디코더에 들어가는 단어들로 만들어지는 Q벡터와 인코더에서 전달받은 K벡터와 V벡터로 최종 단어가 출력되게 된다.

    <a href="https://ibb.co/C68Tjjc"><img src="https://i.ibb.co/m0TMnnj/2021-02-04-19-48-06.png" alt="2021-02-04-19-48-06" border="0"></a>

    디코더에서 self-attention을 만들 때에 인코더와 다른점은 masking처리를 추가해 미래의 정보들을 활용하지 않는다. 다시 말해서 n번째 단어를 입력으로 받았을 때 Q벡터와 나머지 단어들의 K벡터를 곱해서 attention을 만들 때에 1번부터 n번째 단어까지만 K벡터를 곱하게 된다. 



