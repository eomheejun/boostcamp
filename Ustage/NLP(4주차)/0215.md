# Bag-of-Words

- Bag-of-Words란?

  - 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법

    

- Step 1 : 고유 단어를 포함하는 어휘 구축

  예시문장:  “John really really loves this movie“, “Jane really likes this song”

  Vocabulary :  {“John“, “really“, “loves“, “this“, “movie“, “Jane“, “likes“, “song”}

  

- Step 2 : one-hot벡터에 고유 단어 인코딩

  Vocabulary : {“John“, “really“, “loves“, “this“, “movie“, “Jane“, “likes“, “song”}

  위의 단어들을 원핫 벡터로 표현한다. 사전에 등록된 단어의 수가 8개이므로 8차원의 벡터를 만들어 원핫 벡터로 나타낸다.

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/Wfj0XVh/2021-02-15-14-29-00.png" alt="2021-02-15-14-29-00" border="0"></a>

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/SNCdYSN/2021-02-15-14-30-35.png" alt="2021-02-15-14-30-35" border="0"></a>

  단어의 의미를 고려하지 않고 모두 동일한 관계를 가지는 형태로 벡터를 표현하게 된다.



- 예시 각 벡터들의 합 벡터가 입력 문장의 벡터가 된다.

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/t89gT7d/2021-02-15-14-31-59.png" alt="2021-02-15-14-31-59" border="0"></a>



# NaiveBayes Classifier

<a href="https://ibb.co/f8ng132"><img src="https://i.ibb.co/5Yn0MQ6/2021-02-15-14-33-43.png" alt="2021-02-15-14-33-43" border="0"></a>

큰 카테고리의 수를 C (ex. 정치, 경제, 문화...) 라고 설정한다. P(d) 는 argmax에 의해 무시가 가능하기 때문에 마지막과같은 수식이 도출되게 된다.

<a href="https://ibb.co/xFj2J88"><img src="https://i.ibb.co/QDNJrjj/2021-02-15-14-36-04.png" alt="2021-02-15-14-36-04" border="0"></a>

<a href="https://ibb.co/Sv0W9bJ"><img src="https://i.ibb.co/882TSqX/2021-02-15-14-36-36.png" alt="2021-02-15-14-36-36" border="0"></a>



<a href="https://ibb.co/HqZYhXb"><img src="https://i.ibb.co/w4tCcYP/2021-02-15-14-36-56.png" alt="2021-02-15-14-36-56" border="0"></a>



<a href="https://ibb.co/XXL6wFd"><img src="https://i.ibb.co/9ngBMyf/2021-02-15-14-37-26.png" alt="2021-02-15-14-37-26" border="0"></a>





#  Word Embedding

- What is Word Embedding?
  - 단어를 벡터로 표현하는 것
  - 예를들어 cat과 kitty는 비슷한 단어이기 때문에 벡터의 표현이 비슷하다. 다시말해 두 벡터사이의 거리가 짧다.
  - 아예 다른의미를 지닌 hamburger라는 단어는 벡터의 표현이 전혀 다르므로 cat과 kitty벡터와의 거리가 멀다.



- Word2Vec

  - 콘텍스트 워드(인접 단어)에서 단어의 벡터 표현을 훈련하기 위한 알고리즘
  - 비슷한 문맥의 단어들은 비슷한 의미를 가질 것이라고 가정한다.
    - example
      - The cat purrs
      - The cat hunts mice

  - Idea of Word2Vec

    <a href="https://ibb.co/mcwtgwV"><img src="https://i.ibb.co/bKZLDZM/2021-02-15-14-49-21.png" alt="2021-02-15-14-49-21" border="0"></a>

  - How Word2Vec Algorithm Works 

    <a href="https://ibb.co/6sZjshM"><img src="https://i.ibb.co/ThT9hXf/2021-02-15-14-51-32.png" alt="2021-02-15-14-51-32" border="0"></a>

    주어진 학습데이터를 단어별로 나누는 작업을 통해 vocabulary를 만든다. 그 이후 각 단어는  사전의 크기만큼의 차원을 가지는 원핫벡터로 표현된다. 각W1,W2 가중치 행렬을 지나 학습을 통해 출력벡터를 도출하게 되는데 입력 벡터(input vector)와 출력 벡터(output vector) 모두 각각 단어의 의미를 담고 있지만, 이 둘을 조합하면 단어의 의미를 더욱 잘 표현할 수 있다고 알려져 있다.

  

  

- GloVe

  - Word2vec의 한계

    - 저차원 벡터공간에 임베딩된 단어벡터 사이의 유사도를 측정하는 데는 좋은 성능을 가지지만, 사용자가 지정한 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보를 반영하지 못한다.

      

  <a href="https://ibb.co/Z6Lwp4k"><img src="https://i.ibb.co/ynfMcvb/2021-02-15-15-05-55.png" alt="2021-02-15-15-05-55" border="0"></a>

  두 단어가 한 윈도우 내에서 얼마나 동시에 일어났는지에 대한 Pij값을 구해 Log를 취해준 뒤 입력벡터와 출력벡터를 내적한 값과 최대한 가까워 질 수 있도록 Loss Function을 취해주었다. 

  

  <a href="https://ibb.co/n8cGsFG"><img src="https://i.ibb.co/jZMNvSN/2021-02-15-15-16-49.png" alt="2021-02-15-15-16-49" border="0"></a>

  





