# RNN(Recurrent Neural Network)

- Basic structure

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/2W8xpFq/2021-02-16-21-30-48.png" alt="2021-02-16-21-30-48" border="0"></a>

  RNN은 현재의 입력과 이전의 출력으로부터 2가지의 입력을 바탕으로 현재의 출력을 계산한다. 다시 그 출력은 다음 step의 입력으로 들어가 출력을 계산하게 된다. 이 이전의 출력을 hidden vector라고 한다.

  

- Types of RNNs

  <a href="https://ibb.co/K0FGmz2"><img src="https://i.ibb.co/gjz76FS/2021-02-16-22-05-37.png" alt="2021-02-16-22-05-37" border="0"></a>

- Character-level Language Model

  - example of training sequence "hello"

    - Vocabulary: [h, e, l, o]

      <a href="https://ibb.co/Jsh2RM4"><img src="https://i.ibb.co/DRqkDSy/2021-02-16-22-15-49.png" alt="2021-02-16-22-15-49" border="0"></a>

      <a href="https://ibb.co/T0jNmPG"><img src="https://i.ibb.co/YdnVpfS/2021-02-16-22-16-08.png" alt="2021-02-16-22-16-08" border="0"></a>

      

    <a href="https://ibb.co/KrCTmqR"><img src="https://i.ibb.co/fxmjQpB/2021-02-16-22-30-45.png" alt="2021-02-16-22-30-45" border="0"></a>

    이전의 정보가 다음 step의 입력으로 들어가 W와 활성화함수를 거쳐 다음 step으로 넘어가기를 반복하다보면 gradient vanishing/exploding 현상이 일어날 수 있게 된다. 

    

# LSTM

- Basic structure

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/nzDm3x8/2021-02-17-00-31-19.png" alt="2021-02-17-00-31-19" border="0"></a>

  <a href="https://ibb.co/2S9NFF2"><img src="https://i.ibb.co/KDPw55Z/2021-02-17-00-31-58.png" alt="2021-02-17-00-31-58" border="0"></a>

  <a href="https://ibb.co/hZqRd5w"><img src="https://i.ibb.co/WPTWz9j/2021-02-17-00-33-23.png" alt="2021-02-17-00-33-23" border="0"></a>

  <a href="https://ibb.co/m0PBJCm"><img src="https://i.ibb.co/9hdvn9J/2021-02-17-00-33-34.png" alt="2021-02-17-00-33-34" border="0"></a>

  <a href="https://ibb.co/G7bsJ8Z"><img src="https://i.ibb.co/31Lfsjx/2021-02-17-00-33-42.png" alt="2021-02-17-00-33-42" border="0"></a>



# GRU

<a href="https://ibb.co/4FFZzbr"><img src="https://i.ibb.co/grrMN0K/2021-02-17-00-35-07.png" alt="2021-02-17-00-35-07" border="0"></a>

LSTM에서는 출력, 입력, 삭제 게이트라는 3개의 게이트가 존재했다. 반면, GRU에서는 업데이트 게이트와 리셋 게이트 두 가지 게이트만이 존재한다. GRU는 LSTM보다 학습 속도가 빠르다고 알려져있지만 여러 평가에서 GRU는 LSTM과 비슷한 성능을 보인다고 알려져 있다.