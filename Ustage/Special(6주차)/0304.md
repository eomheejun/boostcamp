# 자연어처리를위한 언어모델의 학습과 평가

- 언어모델링(LanguageModeling)

  - 언어 모델링

    <a href="https://ibb.co/Kyx7J02"><img src="https://i.ibb.co/kc20tH4/2021-03-05-23-08-49.png" alt="2021-03-05-23-08-49" border="0"></a>

    <a href="https://ibb.co/9q1cDg8"><img src="https://i.ibb.co/2jCW9hn/2021-03-05-23-08-57.png" alt="2021-03-05-23-08-57" border="0"></a>

    <a href="https://ibb.co/vQ94CXp"><img src="https://i.ibb.co/7RhtxkP/2021-03-05-23-09-02.png" alt="2021-03-05-23-09-02" border="0"></a>

    <a href="https://ibb.co/NrppDBL"><img src="https://i.ibb.co/qC99qQR/2021-03-05-23-09-50.png" alt="2021-03-05-23-09-50" border="0"></a>\

    <a href="https://ibb.co/dsTcNr3"><img src="https://i.ibb.co/TDfTQkn/2021-03-05-23-10-22.png" alt="2021-03-05-23-10-22" border="0"></a>

- BERT의 특징

  - 양방향 (bidirectional) -> GPT같은 경우는 단방향이다.
  - masked language model과 next sentence prediction task를 이용한 pretraining
  - 대규모 데이터셋을 통한 pretraining —> task-specific한 데이터셋에 대해 finetuning