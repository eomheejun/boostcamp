# Quantization

- Quantization(양자화)란 ?

  - 모델의 Weight 를 float형이아닌 int형을 사용하여 모델을 경량화 하는 방법

  - inference 속도를 빠르게 해준다.

    <a href="https://ibb.co/GsdGRQg"><img src="https://i.ibb.co/3fTVcmt/2021-03-18-19-50-16.png" alt="2021-03-18-19-50-16" border="0"></a>

  <a href="https://ibb.co/X5pNKmB"><img src="https://i.ibb.co/c8tMZz5/2021-03-18-19-51-11.png" alt="2021-03-18-19-51-11" border="0"></a>

  <a href="https://ibb.co/18RvCpV"><img src="https://i.ibb.co/XSF2ndB/2021-03-18-19-52-36.png" alt="2021-03-18-19-52-36" border="0"></a>

  <a href="https://ibb.co/NnJXLZ0"><img src="https://i.ibb.co/rfzgZ4B/2021-03-18-19-52-48.png" alt="2021-03-18-19-52-48" border="0"></a>



- 양자화의 종류

  <a href="https://ibb.co/2jdPsVz"><img src="https://i.ibb.co/y8q5Yvz/2021-03-18-19-54-09.png" alt="2021-03-18-19-54-09" border="0"></a>

  양자화의 종류는 무엇을 양자화 할것인가(Weight, 활성함수), 어떻게 할 것인가(Dynamic, Static), 얼만큼 할 것인가(16bit , 8bit , 4bit , 2bit), 언제 할것인가?(학습 이후에, 학습 전에)로 나뉘게 된다.

  <a href="https://ibb.co/XpfyYBz"><img src="https://i.ibb.co/7VxgkcG/2021-03-18-19-56-25.png" alt="2021-03-18-19-56-25" border="0"></a>

  

<a href="https://ibb.co/sHpZkZx"><img src="https://i.ibb.co/DpPZsZc/2021-03-18-20-37-34.png" alt="2021-03-18-20-37-34" border="0"></a>



# Dynamic Quantization

- 신경망에서 Weight를 정수로 양자화하고 활성함수는 inference시 동적으로 양자화하는 방법

  ```
  ##예시 코드
  
  import torch
  
  # define a floating point model
  class M(torch.nn.Module):
      def __init__(self):
          super(M, self).__init__()
          self.fc = torch.nn.Linear(4, 4)
  
      def forward(self, x):
          x = self.fc(x)
          return x
  
  # create a model instance
  model_fp32 = M()
  # create a quantized model instance
  model_int8 = torch.quantization.quantize_dynamic(
      model_fp32,  # the original model
      {torch.nn.Linear},  # a set of layers to dynamically quantize
      dtype=torch.qint8)  # the target dtype for quantized weights
  
  # run the model
  input_fp32 = torch.randn(4, 4, 4, 4)
  res = model_int8(input_fp32)
  ```

  M이라는 model을 선언한 뒤에 양자화 할 계층을 선언 해주고 input을 model에 넣어 결과를 얻어 냈다.

# Static Quantization

- 학습된 모델의 가중치와 활성함수를 양자화 한다. 

  ```
  import torch
  
  class M(torch.nn.Module):
      def __init__(self):
          super(M, self).__init__()
          # QuantStub converts tensors from floating point to quantized
          self.quant = torch.quantization.QuantStub()
          self.conv = torch.nn.Conv2d(1, 1, 1)
          self.relu = torch.nn.ReLU()
          # DeQuantStub converts tensors from quantized to floating point
          self.dequant = torch.quantization.DeQuantStub()
  
      def forward(self, x):
          # manually specify where tensors will be converted from floating
          # point to quantized in the quantized model
          x = self.quant(x)
          x = self.conv(x)
          x = self.relu(x)
          # manually specify where tensors will be converted from quantized
          # to floating point in the quantized model
          x = self.dequant(x)
          return x
  
  # create a model instance
  model_fp32 = M()
  
  # model must be set to eval mode for static quantization logic to work
  model_fp32.eval()
  
  # attach a global qconfig, which contains information about what kind
  # of observers to attach. Use 'fbgemm' for server inference and
  # 'qnnpack' for mobile inference. Other quantization configurations such
  # as selecting symmetric or assymetric quantization and MinMax or L2Norm
  # calibration techniques can be specified here.
  model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')
  
  # Fuse the activations to preceding layers, where applicable.
  # This needs to be done manually depending on the model architecture.
  # Common fusions include `conv + relu` and `conv + batchnorm + relu`
  model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])
  
  # Prepare the model for static quantization. This inserts observers in
  # the model that will observe activation tensors during calibration.
  model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)
  
  # calibrate the prepared model to determine quantization parameters for activations
  # in a real world setting, the calibration would be done with a representative dataset
  input_fp32 = torch.randn(4, 1, 4, 4)
  model_fp32_prepared(input_fp32)
  
  # Convert the observed model to a quantized model. This does several things:
  # quantizes the weights, computes and stores the scale and bias value to be
  # used with each activation tensor, and replaces key operators with quantized
  # implementations.
  model_int8 = torch.quantization.convert(model_fp32_prepared)
  
  # run the model, relevant calculations will happen in int8
  res = model_int8(input_fp32)
  ```

  