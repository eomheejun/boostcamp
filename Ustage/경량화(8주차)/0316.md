# Entropy

- 엔트로피 : 시스템 내 정보의 불확실성 정도를 나타내는 용어

  예를 들어 0과 1만 구분하는 전기 신호로 동전을 5번 던진 결과를 전송해야 한다고 가정해보자. 앞면이라면 1 뒷면이라면 0 이라고 했을 때 앞면인지 질문을 하게 될때 5번만 질문을 하게 되면 된다. 만약 10110을 보낸다면, (앞면,뒷면,앞면,앞면,뒷면)이 되겠습니다. 이렇게 1과 0으로 이루어진 5개의 문자열을 보내면 된다. 

  다른 예로 알파벳 6개를 송신해야 한다고 가정하자. A-Z까지 26개의 알파벳을 1과 0으로 보내는 방법은 알고리즘의 이분탐색처럼 각 글자가 앞쪽 절반(A~M)에 속하는지 뒷쪽 절반(N~Z)에 속하는지를 질문하게 되면 4~5번의 질문만에 한 글자를 추려낼 수 있게 된다. 

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/wRLzjGn/2021-03-16-21-23-06.png" alt="2021-03-16-21-23-06" border="0"></a>

  질문 개수가 약 4.7개이고 4.7번의 질문을 하게 되면 알파벳 한글자를 찾을 수 있게 된다. 이렇게 6글자를 찾아야 되므로 필요한 질문의 갯수는 6 x 4.7 = 28.2번의 질문이 필요하게 된다.

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/cb83DRt/2021-03-16-21-24-31.png" alt="2021-03-16-21-24-31" border="0"></a>

  위와 같은 기능을 하는 문자열을 출력하는 기계 X와 기계 Y가 있다고 가정하자. 기계 X는 (A,B,C,D)를 각각 0.25의 확률로 출력하게 되고 기계 Y는 {A:0.5, B:0.125, C:0.125, D:0.25}의 확률로 출력한다 가정한다.

  기계 X가 문자열 1개를 출력했을 때 어떤 문자열을 출력했는지 찾기 위해서 필요한 질문의 수는 2개이다.  왜냐하면 처음에 (A,B) 중에 있니 (C,D) 중에 있니 라고 반반으로 나눠서 질문하기 때문이다.

  반면 기계 Y의 경우에는 각 문자열을 출력할 확률을 고려해야한다. A가 출력될 확률이 50%이기 때문에 A or B,C,D인지 질문하는 것이 옳은 표현이다. 그 이후 후자가 선택된다면 D의 확률이 나머지중의 50%이기 때문에 B,C or D인지 질문해야 하고 전자를 택했다면 B or C를 질문해야 한다. Y의 경우에는 앞에서 처럼 Binary의 상황이 아니기 때문에 log식으로 계산이 안된다. 그 대신 A가 나타날 확률에 A를 추려내기 위한 질문 개수(처음 1개의 질문만으로 A를 추려냅니다)를 곱한다. 마찬가지로 B가 나타날 확률에 B를 추려내기 위한 질문 개수(3번의 질문으로 추려냅니다)를 곱하고 나머지 C,D에 대해서도 똑같이 하면 질문의 갯수를 구해낼 수 있다.

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/3cKWXh9/2021-03-16-21-35-00.png" alt="2021-03-16-21-35-00" border="0"></a>

  위와 같은 결과로 기계 Y에서 글자하나를 예측하기 위해 필요한 질문의 숫자는 평균적으로 1.75개이다. 기계 X와 기계 Y모두 100글자의 글자를 예측해야 한다고 하면 필요한 질문의 수는 X는 200번 Y는 175번이 필요하게 된다. 다시 말해 Y가 X보다 더 적은 정보량을 생산한다고 볼 수 있다. 이 불확실성의 측정이 Entropy이고 수식적으로 

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/txF3kNf/2021-03-16-21-38-26.png" alt="2021-03-16-21-38-26" border="0"></a>

  위와 같이 표현되게 된다. 위의 수식을 다시 기계 Y에 적용하게 된다면

- A : 0.5 = 1/2

- B: 0.125 = 1/8

- C: 0.125 = 1/8

- D: 0.25 =1/4

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/wJ2bzXf/2021-03-16-21-39-40.png" alt="2021-03-16-21-39-40" border="0"></a>

  가 되고 0.5 * 1 + 0.125 * 3 + 0.125 * 3 + 0.25 * 2 = 1.75가 되어 Entropy는 1.75가 되게 된다.

  

# Cross Entropy

위의 Entropy에서 기계 X와 Y의 Entropy를 수식적으로 구 할수 있었다. Cross Entropy는 위의 Y의 상황을 X와 같이 계산하는 방법이다. Y와 같은 확률분포가 주어졌을 때 문자 하나를 출력하기 위해 X처럼 질문을 하게 된다면 0.5×2+(0.125×2)×2+0.25×2=2라는 수식이 나오게 된다.

<a href="https://imgbb.com/"><img src="https://i.ibb.co/K5WzndZ/2021-03-16-21-48-36.png" alt="2021-03-16-21-48-36" border="0"></a>

위의 수식에서 H(Y,X) = 0.5×2 + (0.125×2) × 2 + 0.25×2 = 2가 되게 된다.

머신러닝에서 분류(classification) 문제에서 주로 Cross Entropy loss함수를 사용하게 된다. 위의 수식에서 Pi가 특정 확률에 대한 참값 또는 목표 확률이고 qi는 우리가 현재까지 학습한 확률 값이다. p=[0.5, 0.125, 0.125, 0.25] 이고, q=[0.25, 0.25, 0.25, 0.25] 가 되게 된다. 우리는 현재 q를 학습하고 있고 q가 p에 가까워 질수록 cross entropy의 값은 작아지게 된다. 다시말해 cross entropy의 최소값은 entropy이다. H(p,q)=H(p)이면 최소가 되게 된다. 즉 p=q일때 최소가 되게 된다. 

# KL-divergence

- **쿨백-라이블러 발산**(Kullback–Leibler divergence, **KLD**)은 두 [확률분포](https://ko.wikipedia.org/wiki/확률분포)의 차이를 계산하는 데에 사용하는 함수로, 어떤 이상적인 분포에 대해, 그 분포를 근사하는 다른 분포를 사용해 샘플링을 한다면 발생할 수 있는 [정보 엔트로피](https://ko.wikipedia.org/wiki/정보_엔트로피) 차이를 계산한다. 상대 엔트로피(relative entropy), 정보 획득량(information gain), 인포메이션 다이버전스(information divergence)라고도 한다.

<a href="https://imgbb.com/"><img src="https://i.ibb.co/JdT4r0n/2021-03-16-22-01-07.png" alt="2021-03-16-22-01-07" border="0"></a>

쉽게 설명해서 위의 확률분포 p와 q의 KL-divergence는 cross entropy - entropy가 되게 된다. 우리가 대개 cross entropy를 minimize 하는 것은, H(p)는 고정된 상수값이기 때문에 KL-divergence를 minimize하는 것과 같은 것을 의미 한다.

- KL-divergence의 특성

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/tLp4hQf/2021-03-16-22-03-40.png" alt="2021-03-16-22-03-40" border="0"></a>

  KL-divergence는 0보다 크거나 같다. cross entropy의 최소값은 entropy이므로 cross entropy-entropy의 값은 당연히 0보다 크거나 같다. 그리고 KL-divergence는 거리개념이 아니기 때문에 KL(p|q) 는 KL(q|p)와 다르게 된다. 만약 거리 개념처럼 사용하고 싶다면 Jensen-Shannon divergence를 사용하면 되고 아래와 같은 수식처럼 사용하면 된다.

  <a href="https://imgbb.com/"><img src="https://i.ibb.co/BgDxQMn/2021-03-16-22-06-27.png" alt="2021-03-16-22-06-27" border="0"></a>

  <a href="https://ibb.co/r21fsSH"><img src="https://i.ibb.co/7t6N45K/2021-03-16-22-07-44.png" alt="2021-03-16-22-07-44" border="0"></a>





- https://hyunw.kim/blog 참조