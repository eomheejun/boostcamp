{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "programmers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "stupid-pound"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Resize, ToTensor, Normalize"
      ],
      "id": "stupid-pound",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eXlaK-_BhIG"
      },
      "source": [
        "!pip install albumentations==0.4.6"
      ],
      "id": "2eXlaK-_BhIG",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb3-o-GF47_Z",
        "outputId": "1bb13ab5-760b-4640-d4bf-10be82f89fc5"
      },
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "id": "Kb3-o-GF47_Z",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/a0/dd40b50aebf0028054b6b35062948da01123d7be38d08b6b1e5435df6363/efficientnet_pytorch-0.7.1.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (1.19.5)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-cp37-none-any.whl size=16443 sha256=4b8bf4ae8753174a1012da33adffd6bf3c7ba83089ce2c1d39f4d262edb35951\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/27/aa/c46d23c4e8cc72d41283862b1437e0b3ad318417e8ed7d5921\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pINLSIvPA4e",
        "outputId": "747637dc-cdb1-42f9-9e4e-185f41113776"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "7pINLSIvPA4e",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "521haTVz5M1K"
      },
      "source": [
        "!unzip /content/drive/MyDrive/train.zip\n",
        "!unzip /content/drive/MyDrive/test.zip"
      ],
      "id": "521haTVz5M1K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "victorian-mention"
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "id": "victorian-mention",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mineral-level"
      },
      "source": [
        "import random\n",
        "\n",
        "random_seed = 42\n",
        "\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)"
      ],
      "id": "mineral-level",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "experimental-stock"
      },
      "source": [
        "test_dir = '/content/test'\n",
        "train_dir = '/content/train'"
      ],
      "id": "experimental-stock",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikSEo7x56yhv"
      },
      "source": [
        "image_list = []\n",
        "for label in os.listdir(train_dir):\n",
        "    for image in os.listdir(os.path.join(train_dir,label)):\n",
        "\n",
        "        image_list.append((os.path.join(train_dir,label,image),label))\n",
        "train_data= np.array(image_list)\n",
        "\n",
        "print(train_data[:10])"
      ],
      "id": "ikSEo7x56yhv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQNQhyMD7jIG"
      },
      "source": [
        "for i in range(len(train_data)):\n",
        "  if train_data[i][1] == 'dog':\n",
        "    train_data[i][1] = 0\n",
        "  elif train_data[i][1] =='elephant':\n",
        "    train_data[i][1] = 1\n",
        "  elif train_data[i][1] == 'giraffe':\n",
        "    train_data[i][1]=2\n",
        "  elif train_data[i][1] == 'guitar':\n",
        "    train_data[i][1]=3\n",
        "  elif train_data[i][1] == 'horse':\n",
        "    train_data[i][1]=4\n",
        "  elif train_data[i][1] == 'house':\n",
        "    train_data[i][1]=5\n",
        "  elif train_data[i][1] == 'person':\n",
        "    train_data[i][1]=6\n",
        "\n",
        "print(train_data[130:150])"
      ],
      "id": "oQNQhyMD7jIG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVlTOEis8XQV",
        "outputId": "34baef9d-1a8f-4d31-bb3d-b2f45ae17263"
      },
      "source": [
        "train_image =[]\n",
        "train_index =[]\n",
        "for i in range(len(train_data)):\n",
        "  train_image.append(train_data[i][0])\n",
        "  train_index.append(int(train_data[i][1]))\n",
        "\n",
        "print(train_image[:5])\n",
        "print(train_index[:5])"
      ],
      "id": "NVlTOEis8XQV",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/train/horse/pic_052.jpg', '/content/train/horse/pic_164.jpg', '/content/train/horse/pic_165.jpg', '/content/train/horse/pic_085.jpg', '/content/train/horse/pic_112.jpg']\n",
            "[4, 4, 4, 4, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0-TsABW8CC_"
      },
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, img_path,img_index, transform):\n",
        "        self.img_path = img_path\n",
        "        self.img_index = img_index\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(self.img_path[index])\n",
        "        label = self.img_index[index]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image,label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path)"
      ],
      "id": "V0-TsABW8CC_",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKuRvd0X817-",
        "outputId": "a85eff35-32e5-4df1-ee24-7c1856937ac6"
      },
      "source": [
        "image_transform = transforms.Compose([\n",
        "    Resize((227, 227), Image.BILINEAR),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2))\n",
        "])"
      ],
      "id": "OKuRvd0X817-",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS2YMDMZ8-Dz"
      },
      "source": [
        "X_train, X_eval, y_train, y_eval  = train_test_split(train_image,train_index, test_size = 0.2)"
      ],
      "id": "nS2YMDMZ8-Dz",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdggV-HO8-GQ"
      },
      "source": [
        "trainset = TrainDataset(X_train,y_train, image_transform)\n",
        "evalset = TrainDataset(X_eval,y_eval, image_transform)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size = 32)\n",
        "val_loader = DataLoader(evalset, batch_size = 32)"
      ],
      "id": "mdggV-HO8-GQ",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL9MlBjr8-Ib"
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.kaiming_uniform_(m.weight)\n",
        "        \n",
        "        \n",
        "model = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "num = model._fc.in_features\n",
        "classifier = nn.Sequential(\n",
        "                           nn.ReLU(),\n",
        "                           nn.Dropout(p=0.5),\n",
        "                           nn.Linear(num,7),\n",
        "                          )\n",
        "model._fc = classifier\n",
        "model.apply(init_weights)\n",
        "model.to(device)"
      ],
      "id": "eL9MlBjr8-Ib",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7nvyp9fDGUU"
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.kaiming_uniform_(m.weight)\n",
        "        \n",
        "\n",
        "model2 = models.resnet50(pretrained=True)\n",
        "num = model2.fc.in_features\n",
        "classifier2 = nn.Sequential(nn.ReLU(),\n",
        "                           nn.Dropout(p=0.5),\n",
        "                           nn.Linear(num,7)\n",
        "                          )\n",
        "model2.fc = classifier2\n",
        "model2.apply(init_weights)\n",
        "model2.to(device)"
      ],
      "id": "j7nvyp9fDGUU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfHmvPHrR98s"
      },
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.kaiming_uniform_(m.weight)\n",
        "        \n",
        "        \n",
        "model3 = EfficientNet.from_pretrained('efficientnet-b3')\n",
        "num = model3._fc.in_features\n",
        "classifier = nn.Sequential(\n",
        "                           nn.ReLU(),\n",
        "                           nn.Dropout(p=0.5),\n",
        "                           nn.Linear(num,7),\n",
        "                          )\n",
        "model3._fc = classifier\n",
        "model3.apply(init_weights)\n",
        "model3.to(device)"
      ],
      "id": "AfHmvPHrR98s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "8zmn6NWC8-NI",
        "outputId": "31e61c33-b407-4cbf-a3fa-42da52da934d"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "a = np.array(train_image)\n",
        "b = np.array(train_index)\n",
        "\n",
        "c = pd.DataFrame(a,columns=['path'])\n",
        "c['label'] = pd.DataFrame(b)\n",
        "c\n",
        "\n",
        "plt.figure(figsize=(6, 3))\n",
        "sns.displot(c, x=\"label\")"
      ],
      "id": "8zmn6NWC8-NI",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7f54462cc550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x216 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgCAYAAACFYaNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAWC0lEQVR4nO3df7BndX3f8efLXQTjr/XHLbO5e7fQSE3QNoteiUImozAmSGzAGUSsFeqQLp1gqqNjIvYP40zpxJkomkxC2QBhaYhAUUdiKYYA0VoLZMFVfkm7VezeZWVXRZA4asB3/7ifLd/gsvcue8/38917n4+ZM/ecz/mcc9/XYV6e/Xw/5/NNVSFJGr9n9C5AklYqA1iSOjGAJakTA1iSOjGAJamT1b0LOBAnnXRSXX/99b3LkKSFZG+NB/UT8Le//e3eJUjS03ZQB7AkHcwMYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4GD+Akq5J8Ocln2/GRSW5Nsi3JVUme2doPbcfb2vkjhq5NknoaxxPwu4B7R44/DFxQVS8BHgLObu1nAw+19gtaP0latgYN4CTrgF8HLm7HAU4ArmldNgOntv1T2jHt/ImtvyQtS0M/AX8M+B3gJ+34RcD3quqxdjwHTLf9aWA7QDv/cOv/DyTZmGRLki27d+8esnZJGtRgAZzkjcCuqrp9Ke9bVZuqaraqZqemppby1pKWgemZ9SQZZJueWb+ktQ65HvDxwG8kORk4DHge8HFgTZLV7Sl3HbCj9d8BzABzSVYDzwe+M2B9kpahB+a285aLvjTIva8657glvd9gT8BVdV5VrauqI4AzgJuq6m3AzcBprdtZwGfa/rXtmHb+pqqqoeqTpN56zAP+XeA9SbYxP8Z7SWu/BHhRa38P8P4OtUnS2IzlK4mq6m+Av2n7XweO3UufHwJvHkc9kjQJfBNOkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpk8ECOMlhSW5L8pUkdyf5UGu/LMk3kmxt24bWniR/mGRbkq8mecVQtUnSJFg94L1/BJxQVY8mOQT4YpL/1s69r6queVL/NwBHte2XgAvbT0lalgZ7Aq55j7bDQ9pW+7jkFODydt0twJoka4eqT5J6G3QMOMmqJFuBXcANVXVrO3V+G2a4IMmhrW0a2D5y+Vxre/I9NybZkmTL7t27hyxfkgY1aABX1eNVtQFYBxyb5OXAecDPA68CXgj87n7ec1NVzVbV7NTU1JLXLEnjMpZZEFX1PeBm4KSq2tmGGX4E/BlwbOu2A5gZuWxda5OkZWnIWRBTSda0/WcBrwe+tmdcN0mAU4G72iXXAme22RCvBh6uqp1D1SdJvQ05C2ItsDnJKuaD/uqq+mySm5JMAQG2Av+29b8OOBnYBvwAeMeAtUlSd4MFcFV9FThmL+0nPEX/As4dqh5JmjS+CSdJnRjAktSJASxJnRjAktSJASxJnRjAktSJASxJnRjAktSJASxJnazIAJ6eWU+SJd+mZ9b3/tMkHUSGXAtiYj0wt523XPSlJb/vVecct+T3lLR8rcgnYEmaBAawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHUyWAAnOSzJbUm+kuTuJB9q7UcmuTXJtiRXJXlmaz+0HW9r548YqjZJmgRDPgH/CDihqn4R2ACclOTVwIeBC6rqJcBDwNmt/9nAQ639gtZPkpatwQK45j3aDg9pWwEnANe09s3AqW3/lHZMO39ikgxVnyT1NugYcJJVSbYCu4AbgP8DfK+qHmtd5oDptj8NbAdo5x8GXjRkfZLU06ABXFWPV9UGYB1wLPDzB3rPJBuTbEmyZffu3QdcoyT1MpZZEFX1PeBm4DXAmiR7vo15HbCj7e8AZgDa+ecD39nLvTZV1WxVzU5NTQ1euyQNZchZEFNJ1rT9ZwGvB+5lPohPa93OAj7T9q9tx7TzN1VVDVWfJPW2euEuT9taYHOSVcwH/dVV9dkk9wBXJvkPwJeBS1r/S4D/nGQb8F3gjAFrk6TuBgvgqvoqcMxe2r/O/Hjwk9t/CLx5qHokadL4JpwkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1InBrAkdWIAS1IngwVwkpkkNye5J8ndSd7V2n8vyY4kW9t28sg15yXZluS+JL82VG2SNAlWD3jvx4D3VtUdSZ4L3J7khnbugqr6g9HOSY4GzgBeBvws8NdJ/mlVPT5gjZLUzWBPwFW1s6ruaPvfB+4FpvdxySnAlVX1o6r6BrANOHao+iSpt7GMASc5AjgGuLU1vTPJV5NcmuQFrW0a2D5y2Rx7CewkG5NsSbJl9+7dA1YtScMaPICTPAf4JPDuqnoEuBD4OWADsBP4yP7cr6o2VdVsVc1OTU0teb2SNC6DBnCSQ5gP3yuq6lMAVfVgVT1eVT8B/pQnhhl2ADMjl69rbZK0LA05CyLAJcC9VfXRkfa1I93eBNzV9q8FzkhyaJIjgaOA24aqT5J6G3IWxPHA24E7k2xtbR8A3ppkA1DA/cA5AFV1d5KrgXuYn0FxrjMgpOVremY9D8xtX7jjMjZYAFfVF4Hs5dR1+7jmfOD8oWqSNDkemNvOWy760pLf96pzjlvyew7FN+EkqRMDWJI6MYAlqRMDWJI6MYAlqRMDWIOZnllPkkG26Zn1vf886YANOQ9YK9xQ04zg4JpqJD0Vn4AlqRMDWJI6MYAlqRMDWJI6MYAlqZNFBXCS4xfTJklavMU+Af/RItskSYu0z3nASV4DHAdMJXnPyKnnAauGLEySlruFXsR4JvCc1u+5I+2PAKcNVZQkrQT7DOCq+jzw+SSXVdU3x1STJK0Ii30V+dAkm4AjRq+pqhOGKEqSVoLFBvB/Af4TcDHg97RJ0hJYbAA/VlUXDlqJJK0wi52G9pdJfivJ2iQv3LMNWpkkLXOLfQI+q/1830hbAf9kacuRpJVjUQFcVUcOXYgkrTSLCuAkZ+6tvaouX9pyJGnlWOwQxKtG9g8DTgTuAAxgSXqaFjsE8dujx0nWAFcOUpEkrRBPdznKvwMcF5akA7DYMeC/ZH7WA8wvwvMLwNVDFaWfNj2zngfmtg9y759dN8OO7f93kHtLemqLHQP+g5H9x4BvVtXcvi5IMsP8GPHhzIf3pqr6eJs/fBXzrzXfD5xeVQ8lCfBx4GTgB8C/rqo79uNvWdb8hmFp+VnUEERblOdrzK+I9gLgx4u47DHgvVV1NPBq4NwkRwPvB26sqqOAG9sxwBuAo9q2EfDNO0nL2mK/EeN04DbgzcDpwK1J9rkcZVXt3PMEW1XfB+4FpoFTgM2t22bg1LZ/CnB5zbsFWJNk7X7+PZJ00FjsEMS/B15VVbsAkkwBfw1cs5iLkxwBHAPcChxeVTvbqW8xP0QB8+E8Osg519p2jrSRZCPzT8isX79+keVL0uRZ7CyIZ+wJ3+Y7i702yXOATwLvrqpHRs9VVfHEh3uLUlWbqmq2qmanpqb251JJmiiLfQK+PsnngE+047cA1y10UZJDmA/fK6rqU635wSRrq2pnG2LYE+w7gJmRy9e1Nklalvb5FJvkJUmOr6r3ARcB/7xt/xPYtMC1AS4B7q2qj46cupYnFvc5C/jMSPuZmfdq4OGRoQpJWnYWegL+GHAeQHuC/RRAkn/Wzv2LfVx7PPB24M4kW1vbB4DfB65OcjbwTeY/1IP5J+qTgW3MT0N7x/7+MZJ0MFkogA+vqjuf3FhVd7YP1p5SVX0RyFOcPnEv/Qs4d4F6JGnZWOiDtDX7OPespSxEklaahQJ4S5J/8+TGJL8J3D5MSZK0Miw0BPFu4NNJ3sYTgTsLPBN405CFSdJyt88ArqoHgeOSvA54eWv+r1V10+CVSdIyt9j1gG8Gbh64FklaUZ7uesCSpANkAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHViAEtSJwawJHUyWAAnuTTJriR3jbT9XpIdSba27eSRc+cl2ZbkviS/NlRdkjQphnwCvgw4aS/tF1TVhrZdB5DkaOAM4GXtmj9JsmrA2iSpu8ECuKq+AHx3kd1PAa6sqh9V1TeAbcCxQ9UmLTfTM+tJsuTb9Mz63n/asra6w+98Z5IzgS3Ae6vqIWAauGWkz1xr+ylJNgIbAdav9z8OCeCBue285aIvLfl9rzrnuCW/p54w7g/hLgR+DtgA7AQ+sr83qKpNVTVbVbNTU1NLXZ8kjc1YA7iqHqyqx6vqJ8Cf8sQwww5gZqTrutYmScvWWAM4ydqRwzcBe2ZIXAuckeTQJEcCRwG3jbM2SRq3wcaAk3wCeC3w4iRzwAeB1ybZABRwP3AOQFXdneRq4B7gMeDcqnp8qNokaRIMFsBV9da9NF+yj/7nA+cPVY8kTRrfhJOkTgxgSerEAJakTgxgSerEAJakTgxgacRQayq4roL2psdaENLEGmpNBXBdBf00n4AlqRMDWJI6MYAlqRMDWJI6MYAlqRMDWJI6MYAlqRMDWJI6MYAlqRMDWJI6MYAlqRMDWJI6MYAlqRMDWJI6MYAlqRMDWJI6MYAlqRMDWJI6MYAlqRMDWJI6GSyAk1yaZFeSu0baXpjkhiT/u/18QWtPkj9Msi3JV5O8Yqi6JGlSDPkEfBlw0pPa3g/cWFVHATe2Y4A3AEe1bSNw4YB1SdJEGCyAq+oLwHef1HwKsLntbwZOHWm/vObdAqxJsnao2iRpEox7DPjwqtrZ9r8FHN72p4HtI/3mWttPSbIxyZYkW3bv3j1cpZI0sG4fwlVVAfU0rttUVbNVNTs1NTVAZZI0HuMO4Af3DC20n7ta+w5gZqTfutYmScvWuAP4WuCstn8W8JmR9jPbbIhXAw+PDFVI0rK0eqgbJ/kE8FrgxUnmgA8Cvw9cneRs4JvA6a37dcDJwDbgB8A7hqpLkibFYAFcVW99ilMn7qVvAecOVYskTSLfhJOkTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSerEAJakTgxgSepkdY9fmuR+4PvA48BjVTWb5IXAVcARwP3A6VX1UI/6JGkcej4Bv66qNlTVbDt+P3BjVR0F3NiOJWnZmqQhiFOAzW1/M3Bqx1okaXC9AriAv0pye5KNre3wqtrZ9r8FHL63C5NsTLIlyZbdu3ePo1ZJGkSXMWDgl6tqR5J/BNyQ5GujJ6uqktTeLqyqTcAmgNnZ2b32kaSDQZcn4Kra0X7uAj4NHAs8mGQtQPu5q0dtkjQuYw/gJM9O8tw9+8CvAncB1wJntW5nAZ8Zd22SNE49hiAOBz6dZM/v/4uquj7J3wJXJzkb+CZweofaJGlsxh7AVfV14Bf30v4d4MRx1yNJvUzSNDRJWlEMYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqxACWpE4MYEnqZOICOMlJSe5Lsi3J+3vXI0lDmagATrIK+GPgDcDRwFuTHN23KkkaxkQFMHAssK2qvl5VPwauBE7pXJMkDSJV1buG/y/JacBJVfWb7fjtwC9V1TtH+mwENrbDlwL3PY1f9WLg2wdY7rhZ83gcjDXDwVn3Sqr521V10pMbVx94PeNVVZuATQdyjyRbqmp2iUoaC2sej4OxZjg467bmyRuC2AHMjByva22StOxMWgD/LXBUkiOTPBM4A7i2c02SNIiJGoKoqseSvBP4HLAKuLSq7h7gVx3QEEYn1jweB2PNcHDWveJrnqgP4SRpJZm0IQhJWjEMYEnqZEUF8MH4mnOSS5PsSnJX71oWK8lMkpuT3JPk7iTv6l3TQpIcluS2JF9pNX+od02LlWRVki8n+WzvWhYjyf1J7kyyNcmW3vUsVpI1Sa5J8rUk9yZ5zQHfc6WMAbfXnP8X8HpgjvkZF2+tqnu6FraAJL8CPApcXlUv713PYiRZC6ytqjuSPBe4HTh1kv+3ThLg2VX1aJJDgC8C76qqWzqXtqAk7wFmgedV1Rt717OQJPcDs1V1UL2EkWQz8N+r6uI2S+tnqup7B3LPlfQEfFC+5lxVXwC+27uO/VFVO6vqjrb/feBeYLpvVftW8x5th4e0beKfTpKsA34duLh3LctZkucDvwJcAlBVPz7Q8IWVFcDTwPaR4zkmPBSWgyRHAMcAt/atZGHtn/JbgV3ADVU18TUDHwN+B/hJ70L2QwF/leT2trTAweBIYDfwZ2245+Ikzz7Qm66kANaYJXkO8Eng3VX1SO96FlJVj1fVBubfwDw2yUQP+SR5I7Crqm7vXct++uWqegXzqx6e24bZJt1q4BXAhVV1DPB3wAF/jrSSAtjXnMeojaN+Eriiqj7Vu5790f5peTPwU4unTJjjgd9oY6pXAick+fO+JS2sqna0n7uATzM/PDjp5oC5kX8VXcN8IB+QlRTAvuY8Ju0DrUuAe6vqo73rWYwkU0nWtP1nMf9h7df6VrVvVXVeVa2rqiOY/+/5pqr6V53L2qckz24fzNL+Cf+rwMTP8KmqbwHbk7y0NZ0IHPCHyhP1KvKQxvia85JK8gngtcCLk8wBH6yqS/pWtaDjgbcDd7YxVYAPVNV1HWtayFpgc5st8wzg6qo6KKZ1HWQOBz49///RrAb+oqqu71vSov02cEV7gPs68I4DveGKmYYmSZNmJQ1BSNJEMYAlqRMDWJI6MYAlqRMDWJI6MYC17CV5dIHzR+zvanNJLmvf4i09bQawJHViAGvFSPKcJDcmuaOtRzu6Gt7qJFe0dV6vSfIz7ZpXJvl8Wzjmc22pTWlJGMBaSX4IvKktBPM64CPttWmAlwJ/UlW/ADwC/FZbz+KPgNOq6pXApcD5HerWMrViXkWWgAD/sa2+9RPmlyM9vJ3bXlX/o+3/OfDvgOuBlwM3tJxeBewca8Va1gxgrSRvA6aAV1bV37dVxA5r5578Tn4xH9h3V9UBf/WMtDcOQWgleT7z6+f+fZLXAf945Nz6ke/4+pfMfyXRfcDUnvYkhyR52Vgr1rJmAGsluQKYTXIncCb/cLnJ+5hfHPxe4AXML7z9Y+A04MNJvgJsBY4bc81axlwNTZI68QlYkjoxgCWpEwNYkjoxgCWpEwNYkjoxgCWpEwNYkjr5f3sNxRm1xpToAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAdUHdSO-UcQ"
      },
      "source": [
        "def get_class_weight(label):\n",
        "    label_unique, count = np.unique(label, return_counts=True)\n",
        "    return [1-c/sum(count) for c in count]\n",
        "\n",
        "result = get_class_weight(train_index)\n",
        "result=torch.Tensor(result)\n",
        "result = result.to(device)"
      ],
      "id": "oAdUHdSO-UcQ",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5mTPDF6-X1W"
      },
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, weight=None,\n",
        "                 gamma=2., reduction='mean'):\n",
        "        nn.Module.__init__(self)\n",
        "        self.weight = weight\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, input_tensor, target_tensor):\n",
        "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
        "        prob = torch.exp(log_prob)\n",
        "        return F.nll_loss(\n",
        "            ((1 - prob) ** self.gamma) * log_prob,\n",
        "            target_tensor,\n",
        "            weight=self.weight,\n",
        "            reduction=self.reduction\n",
        "        )"
      ],
      "id": "_5mTPDF6-X1W",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjyONoOk-Y3l"
      },
      "source": [
        "import torchvision.models as models\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = FocalLoss(weight=result)\n",
        "\n",
        "optimizer = optim.AdamW(model3.parameters(), lr = 3e-4, weight_decay= 1e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 5)"
      ],
      "id": "JjyONoOk-Y3l",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUGqtq5v-chH"
      },
      "source": [
        "def train(epochs , train_loader, val_loader , model , criterion , optimizer,lr_scheduler):\n",
        "    counter = 0\n",
        "    best_val_acc = 0\n",
        "    best_val_loss = np.inf\n",
        "    patience = 5\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        \n",
        "        loss_train_sum = 0\n",
        "        acc_train_sum = 0\n",
        "        \n",
        "        for i , (img , target) in enumerate(train_loader):\n",
        "            img = img.to(device)\n",
        "            target = target\n",
        "            target = target.to(device, dtype=torch.int64)\n",
        "            \n",
        "            y_pred = model(img)\n",
        "            loss = criterion(y_pred, target)\n",
        "        \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train_sum += loss\n",
        "            acc_train_sum += (y_pred.argmax(1) == target).sum().item()/ 32\n",
        "\n",
        "        loss_train_avg = loss_train_sum / len(train_loader)\n",
        "        acc_train_avg = acc_train_sum / len(train_loader)\n",
        "        print(f\" epoch:[{epoch+1}/{epochs}] cost:[{loss_train_avg:.3f}] acc : [{acc_train_avg : .3f}]\")\n",
        "        \n",
        "        \n",
        "        model.eval()\n",
        "        loss_val_sum = 0\n",
        "        acc_val_sum = 0\n",
        "        val_loss_items = []\n",
        "        val_acc_items = []\n",
        "        \n",
        "        for i , (img , target) in enumerate(val_loader):\n",
        "            img = img.to(device)\n",
        "            target = target.to(device, dtype=torch.int64)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                y_pred = model(img)\n",
        "                loss = criterion(y_pred, target)\n",
        "            \n",
        "            loss_val_sum += loss\n",
        "            acc_val_sum += (y_pred.argmax(1) == target).sum().item()/ 32\n",
        "            val_loss_items.append(loss_val_sum)\n",
        "            val_acc_items.append(acc_val_sum)\n",
        "        \n",
        "        \n",
        "        loss_val_avg = loss_val_sum / len(val_loader)\n",
        "        acc_val_avg = acc_val_sum / len(val_loader)\n",
        "        \n",
        "        if loss_val_avg < best_val_loss:\n",
        "            best_val_loss = loss_val_avg\n",
        "        if acc_val_avg > best_val_acc:\n",
        "            print(\"New best model for val accuracy! saving the model..\")\n",
        "            best_val_acc = acc_val_avg\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "        if counter > patience:\n",
        "            print(\"Early Stopping...\")\n",
        "            break\n",
        "            \n",
        "        print(f\" epoch:[{epoch+1}/{epochs}] eval_cost:[{loss_val_avg:.3f}] eval_acc : [{acc_val_avg : .3f}]\")\n",
        "        \n",
        "        lr_scheduler.step()"
      ],
      "id": "bUGqtq5v-chH",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "lV0ClYSj-cjw",
        "outputId": "1943bd2b-e72c-4c60-c44c-3f9857e338d4"
      },
      "source": [
        "train(15 , train_loader , val_loader , model3, criterion ,optimizer,lr_scheduler)"
      ],
      "id": "lV0ClYSj-cjw",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9f35ec1df00f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-75d14a19389a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, train_loader, val_loader, model, criterion, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.33 GiB already allocated; 11.75 MiB free; 13.71 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrVYd9wVIyVg"
      },
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, img_paths, transform):\n",
        "        self.img_paths = img_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(self.img_paths[index])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)"
      ],
      "id": "RrVYd9wVIyVg",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApkGXx_wIFFp"
      },
      "source": [
        "image_paths = glob(test_dir+'/*'+'/*')"
      ],
      "id": "ApkGXx_wIFFp",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubtaATTveeLP"
      },
      "source": [
        "image_paths.sort()\n",
        "image_paths"
      ],
      "id": "ubtaATTveeLP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hZBwNav-pHQ",
        "outputId": "5c686f0a-f6e7-4c3c-f2a3-ed777f91b0e4"
      },
      "source": [
        "submission = pd.read_csv('/content/drive/MyDrive/test_answer_sample_.csv')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    Resize((227, 227), Image.BILINEAR),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
        "])\n",
        "\n",
        "dataset = TestDataset(image_paths, transform)\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "model3.eval()\n",
        "\n",
        "all_predictions = []\n",
        "for images in loader:\n",
        "    with torch.no_grad():\n",
        "        images = images.to(device)\n",
        "        pred = model3(images)\n",
        "        pred = pred.argmax(dim=-1)\n",
        "        all_predictions.extend(pred.cpu().numpy())\n",
        "submission['answer value'] = all_predictions\n",
        "\n",
        "\n",
        "submission.to_csv(os.path.join(test_dir, 'ensemble3.csv'), index=False)\n",
        "print('test inference is done!')"
      ],
      "id": "0hZBwNav-pHQ",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test inference is done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5FHkXkrRNDj"
      },
      "source": [
        "def most_frequent(data):\n",
        "    length = len(data)\n",
        "    arr = [0 for i in range (length)]\n",
        "\n",
        "    for i in range(length):\n",
        "        for j in range(i+1, length):\n",
        "            if data[i] == data[j]:\n",
        "                    arr[i] += 1\n",
        "    max = 0\n",
        "    for i in range(length):\n",
        "        for j in range(i+1, length):\n",
        "            if arr[i] < arr[j]:\n",
        " \n",
        "                max = j\n",
        "    return data[max]"
      ],
      "id": "_5FHkXkrRNDj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OttG80z1MCl4"
      },
      "source": [
        ""
      ],
      "id": "OttG80z1MCl4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z83DtXQRMCoR"
      },
      "source": [
        ""
      ],
      "id": "z83DtXQRMCoR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvQ6i0TX-cmH"
      },
      "source": [
        "submission_csv1 = pd.read_csv('/content/test/ensemble1.csv',index_col = 0)\n",
        "ensemble1 = list(submission_csv1['answer value'])\n",
        "\n",
        "submission_csv2 = pd.read_csv('/content/test/ensemble2.csv',index_col = 0)\n",
        "ensemble2 = list(submission_csv2['answer value'])\n",
        "\n",
        "submission_csv3 = pd.read_csv('/content/test/ensemble3.csv',index_col = 0)\n",
        "ensemble3 = list(submission_csv3['answer value'])\n"
      ],
      "id": "mvQ6i0TX-cmH",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPu0DOTTV-sR"
      },
      "source": [
        "def most_frequent(data):\n",
        "    length = len(data)\n",
        "    arr = [0 for i in range (length)]\n",
        "\n",
        "    for i in range(length):\n",
        "        for j in range(i+1, length):\n",
        "            if data[i] == data[j]:\n",
        "                    arr[i] += 1\n",
        "    max = 0\n",
        "    for i in range(length):\n",
        "        for j in range(i+1, length):\n",
        "            if arr[i] < arr[j]:\n",
        " \n",
        "                max = j\n",
        "    return data[max]"
      ],
      "id": "oPu0DOTTV-sR",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMK31uX1V-uf",
        "outputId": "25a9228d-e2ca-4138-bf0a-b365e6e709ad"
      },
      "source": [
        "final = []\n",
        "\n",
        "for i in range(len(ensemble1)):\n",
        "  data = []\n",
        "  data.append(ensemble1[i])\n",
        "  data.append(ensemble2[i])\n",
        "  data.append(ensemble3[i])\n",
        "  final.append(most_frequent(data))\n",
        "\n",
        "submission['answer value'] = final\n",
        "\n",
        "\n",
        "submission.to_csv(os.path.join(test_dir, 'final.csv'), index=False)\n",
        "print('test inference is done!')"
      ],
      "id": "YMK31uX1V-uf",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test inference is done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnyB_5qoV-xA"
      },
      "source": [
        ""
      ],
      "id": "GnyB_5qoV-xA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI1FjWsSVY1Q",
        "outputId": "86af129a-af57-4e37-d48d-d1280073204a"
      },
      "source": [
        "print(ensemble1)"
      ],
      "id": "TI1FjWsSVY1Q",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 3, 3, 3, 3, 3, 4, 4, 3, 1, 6, 2, 6, 2, 3, 1, 2, 0, 6, 3, 3, 5, 2, 3, 0, 5, 1, 2, 0, 5, 1, 5, 6, 2, 0, 5, 5, 4, 5, 1, 4, 0, 2, 3, 1, 3, 0, 5, 5, 2, 6, 5, 4, 1, 5, 0, 4, 6, 1, 0, 5, 0, 6, 1, 1, 2, 4, 1, 1, 3, 2, 3, 0, 1, 1, 6, 2, 0, 3, 4, 1, 2, 1, 6, 6, 4, 3, 6, 1, 5, 2, 5, 1, 0, 5, 6, 3, 3, 1, 6, 5, 6, 6, 0, 3, 2, 5, 3, 0, 3, 4, 6, 2, 5, 4, 2, 0, 1, 5, 6, 4, 2, 2, 6, 4, 1, 5, 6, 0, 4, 1, 1, 6, 4, 4, 2, 5, 6, 5, 0, 4, 3, 1, 5, 1, 5, 4, 0, 2, 5, 6, 1, 6, 3, 2, 2, 0, 1, 4, 5, 2, 4, 6, 2, 3, 4, 1, 5, 6, 2, 1, 5, 3, 4, 0, 3, 2, 5, 3, 4, 2, 0, 3, 6, 0, 3, 3, 2, 0, 4, 0, 2, 2, 4, 4, 6, 6, 6, 3, 5, 5, 5, 4, 6, 2, 1, 3, 6, 0, 2, 3, 1, 1, 3, 1, 5, 2, 2, 0, 0, 6, 1, 2, 2, 6, 1, 3, 2, 5, 4, 3, 5, 0, 1, 4, 6, 0, 1, 4, 1, 6, 1, 2, 1, 6, 0, 5, 4, 6, 3, 4, 2, 5, 3, 1, 4, 2, 3, 4, 0, 6, 0, 2, 0, 1, 4, 4, 4, 0, 1, 1, 5, 6, 4, 6, 2, 4, 4, 3, 6, 6, 1, 0, 3, 5, 3, 0, 4, 5, 2, 5, 5, 2, 5, 2, 6, 5, 1, 5, 1, 5, 3, 6, 3, 3, 1, 6, 5, 0, 4, 0, 4, 3, 4, 4, 2, 5, 1, 2, 0, 6, 6, 1, 3, 3, 0, 0, 5, 2, 4, 6, 6, 6, 3, 6, 1, 4, 6, 4, 0, 2, 0, 5, 5, 1, 6, 3, 3, 5, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45UAKMis-cq5"
      },
      "source": [
        "a = [1,2,1]"
      ],
      "id": "45UAKMis-cq5",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adolescent-characteristic",
        "outputId": "1d1b7213-5d32-4002-bf1d-56b8744c7f75"
      },
      "source": [
        "b = most_frequent(a)\n",
        "b"
      ],
      "id": "adolescent-characteristic",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "judicial-simulation"
      },
      "source": [
        ""
      ],
      "id": "judicial-simulation",
      "execution_count": null,
      "outputs": []
    }
  ]
}