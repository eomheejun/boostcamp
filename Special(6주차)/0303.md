# 타이타닉 생존자 예측

```
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

titanic_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/boostcamp/test/dataset/train.csv')
titanic_df.head()
```

- Passengerid : 탑승자 데이터 일련번호
- suvived : 생존여부, 0 = 사망, 1 = 생존
- pclass = 티켓의 선실 등급, 1 = 일등석, 2 = 이등석 , 3 = 삼등석
- sex : 성별
- name : 이름
- age : 나이
- sibsp : 같이 탑승한 형제자매 또는 배우자 인원수
- parch : 같이 탑승한 부모님 또는 어린이 인원수
- ticket : 티켓번호
- fare : 요금
- cabin : 선실 번호
- embarked : 중간 정착 항구,  C = Cherbourg, Q = Queenstown, S = Southampton

<a href="https://ibb.co/y4VcGNB"><img src="https://i.ibb.co/qJx3XDF/2021-03-03-23-51-36.png" alt="2021-03-03-23-51-36" border="0"></a>

```
print(titanic_df.info())

>>

<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
None
```

```
# 성별에 따른 생존자 수 예측

titanic_df.groupby(['Sex','Survived'])['Survived'].count()

>>

Sex     Survived
female  0            81
        1           233
male    0           468
        1           109
Name: Survived, dtype: int64
```

여성의 경우 314명중 233명으로 약 74.2%가 생존했고, 남자의 경우 577명 중 109명 으로 약 18.8%가 생존했다

```
# 객실 등급에 따른 각 성별의 생존자 통계

sns.barplot(x='Pclass',y='Survived',hue='Sex',data=titanic_df)
```

<a href="https://imgbb.com/"><img src="https://i.ibb.co/WyjQhbM/2021-03-04-00-02-33.png" alt="2021-03-04-00-02-33" border="0"></a>

여성의 경우 일등석 이등석의 생존확률 차이는 크지 않으나 삼등석의 경우에는 생존확률이 상대적으로 낮았다. 남성의 경우 일등석에서의 생존확률이 높고 이등석,삼등석 같은 경우에는 낮았다.

다음은 나이에 따른 생존 확률이다.

```
def get_category(age):
  cat = ''
  if age<=-1: cat = 'Unknown'
  elif age<=5: cat = 'Baby'
  elif age<=12: cat = 'Child'
  elif age<=18: cat = 'Teenager'
  elif age<=25: cat = 'Student'
  elif age<=35: cat = 'Young Adult'
  elif age<=60: cat = 'Adult'
  else: cat = 'Elderly'

  return cat
plt.figure(figsize=(10,6))

group_names = ['Unknown','Baby','Child','Teenager','Student','Young Adult','Adult','Elderly' ]

titanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x: get_category(x))
sns.barplot(x='Age_cat', y='Survived',hue='Sex',data=titanic_df,order=group_names)
titanic_df.drop('Age_cat', axis=1,inplace=True)
```

<a href="https://ibb.co/yQjFpwW"><img src="https://i.ibb.co/P6HGM7w/2021-03-04-00-18-02.png" alt="2021-03-04-00-18-02" border="0"></a>

남성의 경우 Baby와 Child일 경우에 다른 연령대보다 생존확률이 높았고 여성의 경우 전반적으로 생존확률이 높았으나 Child일 경우에는 생존확률이 떨어졌다.

```
# nan값을 채워준다
titanic_df['Cabin'].fillna('N',inplace=True)
titanic_df['Age'].fillna('N',inplace=True)
titanic_df['Embarked'].fillna('N',inplace=True)
```

문자열 카테고리 피처를 숫자형 카테고리 피처로 변환 한다.

```
from sklearn import preprocessing

def encode_feature(dataDF):
  features= ['Cabin','Sex','Embarked']
  for feature in features:
    le = preprocessing.LabelEncoder()
    le = le.fit(dataDF[feature])
    dataDF[feature] = le.transform(dataDF[feature])
  return dataDF

titanic_df = encode_feature(titanic_df)
titanic_df.head()
```

<a href="https://ibb.co/hdX02LC"><img src="https://i.ibb.co/8xzkPNX/2021-03-04-00-30-05.png" alt="2021-03-04-00-30-05" border="0"></a>

```
##NaN값 처리함수

def fillna(df):
  df['Age'].fillna(df['Age'].mean(), inplace=True)
  df['Cabin'].fillna('N', inplace=True)
  df['Embarked'].fillna('N', inplace=True)
  df['Fare'].fillna(0, inplace=True)
  return df

## 불필요한 속성 제거
def drop_feature(df):
  df.drop(['PassengerId','Name','Ticket'],axis=1,inplace= True)
  return df

## 레이블 인코딩
def format_features(df):
  df['Cabin'] = df['Cabin'].str[:1]
  features = ['Cabin','Sex','Embarked']
  for feature in features:
    le = LabelEncoder()
    le = le.fit(df[feature])
    df[feature] = le.transfrom(df[feature])
  
  return df

## 종합
def transform_features(df):
  df = fillna(df)
  df = drop_feature(df)
  df = format_features(df)
  return df
```

```
x_titanic_df
```

<a href="https://ibb.co/TmXfxGb"><img src="https://i.ibb.co/QrBg3VC/2021-03-04-18-13-05.png" alt="2021-03-04-18-13-05" border="0"></a>

```
y_titanic_df

>>

0      0
1      1
2      1
3      1
4      0
      ..
886    0
887    1
888    0
889    1
890    0
Name: Survived, Length: 891, dtype: int64
```

```
# train set과 test set 분할

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x_titanic_df,y_titanic_df,test_size=0.2,random_state=11)
```

```
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 결정트리, Random Forest, 로지스틱 회귀

dt_clf = DecisionTreeClassifier(random_state=11)
rf_clf = RandomForestClassifier(random_state=11)
lr_clf = LogisticRegression()

#DecisionTreeClassifier 학습/예측/평가

dt_clf.fit(x_train,y_train)
dt_pred = dt_clf.predict(x_test)
print('DecisionTreeClassifier 정확도:{0:.4f}'.format(accuracy_score(y_test,dt_pred)))

#RandomForestClassifier 학습/예측/평가

rf_clf.fit(x_train,y_train)
rf_pred = rf_clf.predict(x_test)
print('RandomForestClassifier 정확도: {0:.4f}'.format(accuracy_score(y_test,rf_pred)))

#LogisticRegression 학습/예측/평가

lr_clf.fit(x_train,y_train)
lr_pred = lr_clf.predict(x_test)
print('LogisticRegression 정확도: {0:.4f}'.format(accuracy_score(y_test,lr_pred)))

>>

DecisionTreeClassifier 정확도: 0.7877
RandomForestClassifier 정확도: 0.8547
LogisticRegression 정확도: 0.8492
```

```
from sklearn.model_selection import KFold

def exec_kfold(clf,folds=5):
  kfold = KFold(n_splits=folds)
  scores =[]
  for iter_count,(train_index,test_index) in enumerate(kfold.split(x_titanic_df)):
    x_train, x_test = x_titanic_df.values[train_index], x_titanic_df.values[test_index]
    y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]

    clf.fit(x_train,y_train)
    predictions = clf.predict(x_test)
    acc = accuracy_score(y_test,predictions)
    scores.append(acc)
    print("교차검증 {0} 정확도: {1:.4f}".format(iter_count,acc))

  mean_score =np.mean(scores)
  print('평균 정확도: {0:.4f}'.format(mean_score))

exec_kfold(dt_clf,folds=5)

>>

교차검증 0 정확도: 0.7542
교차검증 1 정확도: 0.7809
교차검증 2 정확도: 0.7865
교차검증 3 정확도: 0.7697
교차검증 4 정확도: 0.8202
평균 정확도: 0.7823
```

```
from sklearn.model_selection import cross_val_score

scores = cross_val_score(dt_clf,x_titanic_df,y_titanic_df,cv=5)
for iter_count,acc in enumerate(scores):
  print("교차검증 {0} 정확도:{1:.4f}".format(iter_count,acc))

mean_score =np.mean(scores)
print('평균 정확도: {0:.4f}'.format(mean_score))

>>

교차검증 0 정확도:0.7430
교차검증 1 정확도:0.7753
교차검증 2 정확도:0.7921
교차검증 3 정확도:0.7865
교차검증 4 정확도:0.8427
평균 정확도: 0.7879
```

Kfold와 cross_val_score를 사용했을 때 정확도가 다른 이유는 cross_val_score는 stratified kfold를 사용하기 때문이다.

```
##GridSearchCV사용

from sklearn.model_selection import GridSearchCV

param = {'max_depth':[2,3,5,10],'min_samples_split':[2,3,5],'min_samples_leaf':[1,5,8]}

grid_dclf = GridSearchCV(dt_clf,param_grid=param,scoring='accuracy',cv=5)
grid_dclf.fit(x_train,y_train)

print('GridSearchCV 최적 하이퍼 파라미터:',grid_dclf.best_params_)
print('GridSearchCV 최고 정확도 {0:.4f}'.format(grid_dclf.best_score_))
best_dclf=grid_dclf.best_estimator_

dpred = best_dclf.predict(x_test)
acc = accuracy_score(y_test,dpred)

print('정확도:{0:.4f}'.format(acc))

>>

GridSearchCV 최적 하이퍼 파라미터: {'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 2}
GridSearchCV 최고 정확도 0.7992
정확도:0.8715
```

