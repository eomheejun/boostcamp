# Pruning

<a href="https://ibb.co/fqMGDHp"><img src="https://i.ibb.co/S5N7fBn/2021-03-17-21-59-13.png" alt="2021-03-17-21-59-13" border="0"></a>

위의 사진처럼 사람은 성장하면서 잘 않쓰이는 synapses를 잘라내게 되어 성인이 되면 어릴 때의 비해 평균적으로 절반 가까운 synapses를 가지게 된다. 딥러닝에서 마찬가지로 모델의 기능을 최대한 방해하지 않는 선에서 학습 후 불필요한 부분을 제거하는 방식으로, 가중치의 크기에 기반한제거, 어텐션 헤드 제거, 레이어 제거 등 여러 방법으로 불필요한 부분을 제거하여 모델을 압축한다.

<a href="https://ibb.co/QfZ8fLG"><img src="https://i.ibb.co/pxgWxcp/2021-03-17-22-01-50.png" alt="2021-03-17-22-01-50" border="0"></a>

학습이 끝난 뒤 pruning을 하게 되면 predict하는데 있어서 속도를 줄일 수 있게 된다. 반면 loss에 대한 정보를 잃게 된다는 단점이 있다. 

<a href="https://ibb.co/WftzRvj"><img src="https://i.ibb.co/9Wt4dHS/2021-03-17-22-03-30.png" alt="2021-03-17-22-03-30" border="0"></a>

위와 같이 연결된 신경망에서 불필요한 부분을 제거하게 된다. 주로 weight 값이 0인 부분을 제거를 하게 되는데 (weight 가 0이면 불필요한 부분이라고 판단) 제거 하고 나면 오른쪽과 같은 그림을 가지게 되는데 그 숫자가 1/10의 크기로 줄게 되어(엄청나게 많이 제거) 실제로 나타나는 그림은 처음의 그림에서 굉장히 작은 갯수의 weight 들만 남게 된다. 다시말해 predict하는데 있어서 계산량이 적어져 속도가 빠르게 된다.

<a href="https://ibb.co/zfgG8pM"><img src="https://i.ibb.co/JkT5xV1/2021-03-17-22-05-59.png" alt="2021-03-17-22-05-59" border="0"></a>

왼쪽은 pruning 오른쪽은 dropout이다. 비슷해보이지만 내용은 다르다. 우선 pruning은 한번 제거하게 되면 복원이 안된다. 원본의 모델 모형을 불필요한 부분을 제거하면서 경량화를 시킨것이다. 반면 dropout은 overfit을 방지하기 위해 random하게 노드들을 껐다 켰다를 반복하게 되는 것이다. 다시 말해 원본 모델의 모형은 바뀌지 않게 된다는 차이점이 있다.

<a href="https://ibb.co/Vj0vNSG"><img src="https://i.ibb.co/Pms5Yhy/2021-03-17-22-08-06.png" alt="2021-03-17-22-08-06" border="0"></a>